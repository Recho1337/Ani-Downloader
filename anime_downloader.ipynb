{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cinichi/Ani-Downloader/blob/main/anime_downloader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NioTUUOYM4N7"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# ================================================================\n",
        "# ðŸ“œ DISCLAIMER\n",
        "# Personal / educational use only.\n",
        "# Respect copyright laws and the AnimeKai/host site terms of use.\n",
        "# Do NOT use this notebook for commercial or infringing purposes.\n",
        "# ================================================================\n",
        "\n",
        "# ðŸŽ¬ AnimeKai Episode Downloader & Merger (FIXED VERSION)\n",
        "# Fixes: Subtitle embedding for Soft Sub/Dub, Single episode upload, Episode naming\n",
        "\n",
        "# @title ðŸ”§ Install Dependencies { display-mode: \"form\" }\n",
        "print(\"ðŸ“¦ Installing required packages...\")\n",
        "!pip install -q requests beautifulsoup4 cloudscraper m3u8 pycryptodome tqdm yt-dlp\n",
        "!apt-get -qq install -y ffmpeg aria2 > /dev/null 2>&1\n",
        "print(\"âœ… All dependencies installed!\\n\")\n",
        "\n",
        "# @title âš™ï¸ Configuration { display-mode: \"form\" }\n",
        "\n",
        "#@markdown ### ðŸ”— Anime URL\n",
        "anime_url = \"https://anikai.to/watch/my-dress-up-darling-season-2-p8em\"  # @param {type:\"string\"}\n",
        "\n",
        "#@markdown ### ðŸ“º Episode Selection\n",
        "download_mode = \"All Episodes\"  # @param [\"All Episodes\", \"Episode Range\", \"Single Episode\"]\n",
        "\n",
        "#@markdown Single episode (accepts things like \"12.5\", \"SP\"):\n",
        "single_episode = \"1\"  # @param {type:\"string\"}\n",
        "\n",
        "#@markdown Episode range (interpreted numerically when possible):\n",
        "start_episode = \"1\"  # @param {type:\"string\"}\n",
        "end_episode   = \"2\"  # @param {type:\"string\"}\n",
        "\n",
        "#@markdown ### ðŸŽ¥ Quality & Audio Settings\n",
        "video_quality = \"720p\"  # @param [\"1080p\", \"720p\", \"480p\", \"360p\"]\n",
        "\n",
        "# NOTE: \"Dub (with subs)\" is AnimeKai's dub type; not true dual-audio.\n",
        "prefer_type = \"Soft Sub\"  # @param [\"Hard Sub\", \"Soft Sub\", \"Dub (with subs)\"]\n",
        "prefer_server = \"Server 1\"  # @param [\"Server 1\", \"Server 2\"]\n",
        "\n",
        "#@markdown ### ðŸ“¥ Download Settings\n",
        "download_method = \"yt-dlp\"  # @param [\"yt-dlp\", \"aria2\", \"chunks\", \"ffmpeg\"]\n",
        "\n",
        "chunk_size_mb      = 15    # @param {type:\"slider\", min:1, max:20, step:1}\n",
        "max_workers        = 15    # @param {type:\"slider\", min:1, max:16, step:1}\n",
        "max_retries        = 7    # @param {type:\"slider\", min:1, max:10, step:1}\n",
        "connection_timeout = 300  # @param {type:\"slider\", min:60, max:600, step:30}\n",
        "\n",
        "#@markdown ### ðŸ”— Merge Settings\n",
        "merge_episodes = True  # @param {type:\"boolean\"}\n",
        "season_number  = 0     # @param {type:\"integer\"}  # 0 = auto-detect\n",
        "keep_individual_files = False  # @param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ### ðŸ“¤ Upload Settings\n",
        "upload_destination = \"Both\"  # @param [\"GoFile.io Only\", \"Google Drive Only\", \"Both\", \"None (Keep Local)\"]\n",
        "upload_merged_only = True    # @param {type:\"boolean\"}\n",
        "\n",
        "print(\"âœ… Configuration set!\")\n",
        "print(f\"ðŸ“¥ Download method: {download_method}\")\n",
        "print(f\"âš™ï¸ Workers: {max_workers} | Chunk size: {chunk_size_mb}MB\")\n",
        "print(f\"ðŸ”„ Max retries: {max_retries} | Timeout: {connection_timeout}s\")\n",
        "if merge_episodes:\n",
        "    print(f\"ðŸ”— Merge enabled | Keep files: {keep_individual_files}\")\n",
        "\n",
        "# ================================================================\n",
        "# Imports & globals\n",
        "# ================================================================\n",
        "\n",
        "import requests\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "import subprocess\n",
        "from typing import List, Optional, Tuple, Dict, Any\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import cloudscraper\n",
        "from urllib.parse import urlparse\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "def log(level: str, msg: str) -> None:\n",
        "    print(f\"[{level}] {msg}\")\n",
        "\n",
        "BASE_URL = \"https://animekai.to\"\n",
        "scraper = cloudscraper.create_scraper(\n",
        "    browser={\"browser\": \"chrome\", \"platform\": \"windows\", \"desktop\": True}\n",
        ")\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\",\n",
        "    \"Referer\": BASE_URL,\n",
        "    \"Accept\": \"*/*\",\n",
        "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "    \"Connection\": \"keep-alive\",\n",
        "}\n",
        "\n",
        "RETRY_CONFIG = {\n",
        "    \"max_retries\": max_retries,\n",
        "    \"sleep_between\": 3,\n",
        "    \"timeout\": connection_timeout,\n",
        "}\n",
        "\n",
        "# ================================================================\n",
        "# enc-dec helpers\n",
        "# ================================================================\n",
        "\n",
        "def call_enc_dec_api(endpoint: str, payload: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
        "    base = \"https://enc-dec.app/api\"\n",
        "    url = f\"{base}/{endpoint}\"\n",
        "    try:\n",
        "        if endpoint.startswith(\"enc-\"):\n",
        "            text = payload.get(\"text\", \"\")\n",
        "            resp = scraper.get(f\"{url}?text={text}\", headers=HEADERS, timeout=15)\n",
        "        else:\n",
        "            resp = scraper.post(\n",
        "                url,\n",
        "                headers={\"Content-Type\": \"application/json\"},\n",
        "                data=json.dumps(payload),\n",
        "                timeout=30,\n",
        "            )\n",
        "        resp.raise_for_status()\n",
        "        return resp.json()\n",
        "    except Exception as e:\n",
        "        log(\"ERROR\", f\"enc-dec API '{endpoint}' failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def enc_kai(text: str) -> Optional[str]:\n",
        "    data = call_enc_dec_api(\"enc-kai\", {\"text\": text})\n",
        "    if not data or \"result\" not in data:\n",
        "        log(\"ERROR\", \"Failed to get enc-kai result.\")\n",
        "        return None\n",
        "    return data[\"result\"]\n",
        "\n",
        "def dec_kai(text: str) -> Optional[Dict[str, Any]]:\n",
        "    data = call_enc_dec_api(\"dec-kai\", {\"text\": text})\n",
        "    if not data or \"result\" not in data:\n",
        "        log(\"ERROR\", \"Failed to decode dec-kai payload.\")\n",
        "        return None\n",
        "    return data[\"result\"]\n",
        "\n",
        "def dec_mega(text: str, agent: str) -> Optional[Dict[str, Any]]:\n",
        "    data = call_enc_dec_api(\"dec-mega\", {\"text\": text, \"agent\": agent})\n",
        "    if not data or \"result\" not in data:\n",
        "        log(\"ERROR\", \"Failed to decode dec-mega payload.\")\n",
        "        return None\n",
        "    return data[\"result\"]\n",
        "\n",
        "# ================================================================\n",
        "# Anime info & episodes\n",
        "# ================================================================\n",
        "\n",
        "def get_anime_details(url: str) -> Tuple[Optional[str], str]:\n",
        "    try:\n",
        "        r = scraper.get(url, headers=HEADERS, timeout=30)\n",
        "        r.raise_for_status()\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "\n",
        "        anime_div = soup.select_one(\"div[data-id]\")\n",
        "        anime_id = anime_div.get(\"data-id\") if anime_div else None\n",
        "\n",
        "        title_elem = (\n",
        "            soup.select_one(\"div.title-wrapper h1.title span\")\n",
        "            or soup.select_one(\"h1.title\")\n",
        "            or soup.select_one(\".anime-title\")\n",
        "        )\n",
        "        title = title_elem.get(\"title\") if title_elem and title_elem.get(\"title\") else (\n",
        "            title_elem.text.strip() if title_elem else \"Unknown\"\n",
        "        )\n",
        "        title = re.sub(r'[<>:\"/\\\\|?*]', \"\", title)\n",
        "        return anime_id, title\n",
        "    except Exception as e:\n",
        "        log(\"ERROR\", f\"Error getting anime details: {e}\")\n",
        "        return None, \"Unknown\"\n",
        "\n",
        "def detect_season_from_title(title: str) -> int:\n",
        "    patterns = [\n",
        "        r\"[Ss]eason\\s+(\\d+)\",\n",
        "        r\"[Ss](\\d+)\",\n",
        "        r\"(\\d+)(?:st|nd|rd|th)\\s+[Ss]eason\",\n",
        "        r\"\\s+(\\d+)$\",\n",
        "        r\"Part\\s+(\\d+)\",\n",
        "        r\"Cour\\s+(\\d+)\",\n",
        "    ]\n",
        "    for p in patterns:\n",
        "        m = re.search(p, title)\n",
        "        if m:\n",
        "            return int(m.group(1))\n",
        "    return 1\n",
        "\n",
        "def safe_episode_key(ep_id: str) -> Tuple[int, float]:\n",
        "    m = re.match(r\"(\\d+)(?:\\.(\\d+))?\", ep_id)\n",
        "    if m:\n",
        "        main = int(m.group(1))\n",
        "        frac = float(f\"0.{m.group(2)}\") if m.group(2) else 0.0\n",
        "        return main, frac\n",
        "    return (10**9, 0.0)\n",
        "\n",
        "def get_episode_list(anime_id: str) -> List[Dict[str, Any]]:\n",
        "    try:\n",
        "        enc = enc_kai(anime_id)\n",
        "        if not enc:\n",
        "            return []\n",
        "        url = f\"{BASE_URL}/ajax/episodes/list?ani_id={anime_id}&_={enc}\"\n",
        "        r = scraper.get(url, headers=HEADERS, timeout=30)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        html = data.get(\"result\", \"\")\n",
        "        if not html:\n",
        "            return []\n",
        "\n",
        "        soup = BeautifulSoup(html, \"html.parser\")\n",
        "        episodes: List[Dict[str, Any]] = []\n",
        "        for ep in soup.select(\"div.eplist a\"):\n",
        "            token = ep.get(\"token\", \"\")\n",
        "            ep_id = ep.get(\"num\", \"\").strip()\n",
        "            langs = ep.get(\"langs\", \"0\")\n",
        "            try:\n",
        "                langs_int = int(langs)\n",
        "            except ValueError:\n",
        "                langs_int = 0\n",
        "            if langs_int == 1:\n",
        "                subdub = \"Sub\"\n",
        "            elif langs_int == 3:\n",
        "                subdub = \"Dub & Sub\"\n",
        "            else:\n",
        "                subdub = \"\"\n",
        "\n",
        "            episodes.append(\n",
        "                {\n",
        "                    \"id\": ep_id,\n",
        "                    \"sort_key\": safe_episode_key(ep_id),\n",
        "                    \"token\": token,\n",
        "                    \"subdub\": subdub,\n",
        "                    \"title\": f\"Episode {ep_id}\",\n",
        "                }\n",
        "            )\n",
        "        episodes.sort(key=lambda e: e[\"sort_key\"])\n",
        "        return episodes\n",
        "    except Exception as e:\n",
        "        log(\"ERROR\", f\"Error getting episodes: {e}\")\n",
        "        return []\n",
        "\n",
        "# ================================================================\n",
        "# Server selection\n",
        "# ================================================================\n",
        "\n",
        "def normalize(s: str) -> str:\n",
        "    return s.strip().lower()\n",
        "\n",
        "def server_matches_pref(server_name: str, preferred: str) -> bool:\n",
        "    s = normalize(server_name)\n",
        "    p = normalize(preferred)\n",
        "    return p in s or s in p\n",
        "\n",
        "def get_video_servers(token: str) -> List[Dict[str, str]]:\n",
        "    try:\n",
        "        enc = enc_kai(token)\n",
        "        if not enc:\n",
        "            return []\n",
        "        url = f\"{BASE_URL}/ajax/links/list?token={token}&_={enc}\"\n",
        "        r = scraper.get(url, headers=HEADERS, timeout=30)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        html = data.get(\"result\", \"\")\n",
        "        if not html:\n",
        "            return []\n",
        "        soup = BeautifulSoup(html, \"html.parser\")\n",
        "        servers: List[Dict[str, str]] = []\n",
        "        for type_div in soup.select(\"div.server-items[data-id]\"):\n",
        "            type_id = type_div.get(\"data-id\", \"\")\n",
        "            for server in type_div.select(\"span.server[data-lid]\"):\n",
        "                server_id = server.get(\"data-lid\", \"\")\n",
        "                server_name = server.text.strip()\n",
        "                servers.append(\n",
        "                    {\"type\": type_id, \"server_id\": server_id, \"server_name\": server_name}\n",
        "                )\n",
        "        return servers\n",
        "    except Exception as e:\n",
        "        log(\"ERROR\", f\"Error getting servers: {e}\")\n",
        "        return []\n",
        "\n",
        "def choose_server(\n",
        "    servers: List[Dict[str, str]],\n",
        "    prefer_type_label: str,\n",
        "    prefer_server_name: str,\n",
        ") -> Optional[Dict[str, str]]:\n",
        "    type_map = {\n",
        "        \"Hard Sub\":        \"sub\",\n",
        "        \"Soft Sub\":        \"softsub\",\n",
        "        \"Dub (with subs)\": \"dub\",\n",
        "    }\n",
        "    prefer_type_id = type_map.get(prefer_type_label, \"softsub\")\n",
        "\n",
        "    if not servers:\n",
        "        return None\n",
        "\n",
        "    cand = [\n",
        "        s for s in servers\n",
        "        if s[\"type\"] == prefer_type_id and server_matches_pref(s[\"server_name\"], prefer_server_name)\n",
        "    ]\n",
        "    if cand:\n",
        "        return cand[0]\n",
        "\n",
        "    cand = [s for s in servers if server_matches_pref(s[\"server_name\"], prefer_server_name)]\n",
        "    if cand:\n",
        "        return cand[0]\n",
        "\n",
        "    cand = [s for s in servers if s[\"type\"] == prefer_type_id]\n",
        "    if cand:\n",
        "        return cand[0]\n",
        "\n",
        "    return servers[0]\n",
        "\n",
        "# ================================================================\n",
        "# Resolve video URL with subtitle tracks (FIXED)\n",
        "# ================================================================\n",
        "\n",
        "def get_video_data(server_id: str) -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Returns dict with:\n",
        "      - video_url: main video URL\n",
        "      - subtitles: list of subtitle tracks [{'url': ..., 'lang': ...}]\n",
        "    \"\"\"\n",
        "    try:\n",
        "        enc = enc_kai(server_id)\n",
        "        if not enc:\n",
        "            return None\n",
        "        url = f\"{BASE_URL}/ajax/links/view?id={server_id}&_={enc}\"\n",
        "        r = scraper.get(url, headers=HEADERS, timeout=30)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        encoded_link = data.get(\"result\", \"\")\n",
        "        if not encoded_link:\n",
        "            return None\n",
        "\n",
        "        dec = dec_kai(encoded_link)\n",
        "        if not dec:\n",
        "            return None\n",
        "        iframe_url = dec.get(\"url\", \"\")\n",
        "        if not iframe_url:\n",
        "            return None\n",
        "\n",
        "        parsed = urlparse(iframe_url)\n",
        "        token = parsed.path.split(\"/\")[-1]\n",
        "        media_url = f\"{parsed.scheme}://{parsed.netloc}/media/{token}\"\n",
        "        r2 = scraper.get(media_url, headers=HEADERS, timeout=30)\n",
        "        r2.raise_for_status()\n",
        "        j2 = r2.json()\n",
        "        mega_token = j2.get(\"result\", \"\")\n",
        "        if not mega_token:\n",
        "            return None\n",
        "\n",
        "        mega = dec_mega(mega_token, HEADERS[\"User-Agent\"])\n",
        "        if not mega:\n",
        "            return None\n",
        "\n",
        "        sources = mega.get(\"sources\", [])\n",
        "        if not sources:\n",
        "            return None\n",
        "\n",
        "        video_url = sources[0].get(\"file\", \"\")\n",
        "\n",
        "        # Extract subtitle tracks (VTT only)\n",
        "        subtitle_tracks = []\n",
        "        tracks = mega.get(\"tracks\", [])\n",
        "        for track in tracks:\n",
        "            if track.get(\"kind\") == \"captions\" and track.get(\"file\", \"\").endswith(\".vtt\"):\n",
        "                subtitle_tracks.append({\n",
        "                    \"url\": track[\"file\"],\n",
        "                    \"lang\": track.get(\"label\", \"Unknown\")\n",
        "                })\n",
        "\n",
        "        return {\n",
        "            \"video_url\": video_url,\n",
        "            \"subtitles\": subtitle_tracks\n",
        "        }\n",
        "    except Exception as e:\n",
        "        log(\"ERROR\", f\"Error getting video data: {e}\")\n",
        "        return None\n",
        "\n",
        "# ================================================================\n",
        "# Download with subtitle embedding (FIXED)\n",
        "# ================================================================\n",
        "\n",
        "def download_with_ytdlp(url: str, output_file: str, episode_label: str, subtitles: List[Dict] = None) -> bool:\n",
        "    \"\"\"Download with yt-dlp, embedding subtitles if available.\"\"\"\n",
        "    try:\n",
        "        log(\"INFO\", f\"Using yt-dlp -> {os.path.basename(output_file)}\")\n",
        "\n",
        "        # Base command\n",
        "        cmd = [\n",
        "            \"yt-dlp\",\n",
        "            url,\n",
        "            \"-o\",\n",
        "            output_file,\n",
        "            \"--no-warnings\",\n",
        "            \"--no-check-certificate\",\n",
        "            \"--concurrent-fragments\",\n",
        "            str(max_workers),\n",
        "            \"--retries\",\n",
        "            str(RETRY_CONFIG[\"max_retries\"]),\n",
        "            \"--fragment-retries\",\n",
        "            str(RETRY_CONFIG[\"max_retries\"]),\n",
        "            \"--socket-timeout\",\n",
        "            str(RETRY_CONFIG[\"timeout\"]),\n",
        "            \"--user-agent\",\n",
        "            HEADERS[\"User-Agent\"],\n",
        "            \"--referer\",\n",
        "            BASE_URL,\n",
        "            \"--newline\",\n",
        "        ]\n",
        "\n",
        "        # Add subtitle handling if available\n",
        "        if subtitles:\n",
        "            log(\"INFO\", f\"Found {len(subtitles)} subtitle track(s)\")\n",
        "            # Download video and subs separately, then merge\n",
        "            temp_video = output_file.replace(\".mp4\", \"_temp.mp4\")\n",
        "            cmd_copy = cmd.copy()\n",
        "            cmd_copy[cmd_copy.index(\"-o\") + 1] = temp_video\n",
        "\n",
        "            # Download video first\n",
        "            proc = subprocess.Popen(\n",
        "                cmd_copy,\n",
        "                stdout=subprocess.PIPE,\n",
        "                stderr=subprocess.STDOUT,\n",
        "                universal_newlines=True,\n",
        "                bufsize=1,\n",
        "            )\n",
        "            with tqdm(\n",
        "                total=100,\n",
        "                unit=\"%\",\n",
        "                desc=f\"Ep {episode_label} (video)\",\n",
        "                bar_format=\"{desc}: {percentage:3.0f}%|{bar}| {elapsed}\",\n",
        "            ) as pbar:\n",
        "                last = 0.0\n",
        "                for line in proc.stdout:\n",
        "                    m = re.search(r\"\\[download\\]\\s+(\\d+\\.?\\d*)%\", line)\n",
        "                    if m:\n",
        "                        cur = float(m.group(1))\n",
        "                        delta = cur - last\n",
        "                        if delta > 0:\n",
        "                            pbar.update(delta)\n",
        "                            last = cur\n",
        "            proc.wait()\n",
        "\n",
        "            if proc.returncode != 0 or not os.path.exists(temp_video):\n",
        "                log(\"ERROR\", f\"Video download failed\")\n",
        "                return False\n",
        "\n",
        "            # Download subtitles\n",
        "            sub_files = []\n",
        "            for idx, sub in enumerate(subtitles):\n",
        "                sub_path = output_file.replace(\".mp4\", f\"_sub{idx}.vtt\")\n",
        "                try:\n",
        "                    log(\"INFO\", f\"Downloading subtitle: {sub['lang']}\")\n",
        "                    r = scraper.get(sub['url'], headers=HEADERS, timeout=30)\n",
        "                    r.raise_for_status()\n",
        "                    with open(sub_path, 'wb') as f:\n",
        "                        f.write(r.content)\n",
        "                    sub_files.append((sub_path, sub['lang']))\n",
        "                except Exception as e:\n",
        "                    log(\"WARN\", f\"Failed to download subtitle {sub['lang']}: {e}\")\n",
        "\n",
        "            # Merge video + subtitles with ffmpeg\n",
        "            if sub_files:\n",
        "                log(\"INFO\", f\"Embedding {len(sub_files)} subtitle(s)...\")\n",
        "                ffmpeg_cmd = [\"ffmpeg\", \"-i\", temp_video]\n",
        "\n",
        "                # Add subtitle inputs\n",
        "                for sub_file, _ in sub_files:\n",
        "                    ffmpeg_cmd.extend([\"-i\", sub_file])\n",
        "\n",
        "                # Map video and audio\n",
        "                ffmpeg_cmd.extend([\"-map\", \"0:v\", \"-map\", \"0:a\"])\n",
        "\n",
        "                # Map and set metadata for each subtitle\n",
        "                for idx, (_, lang) in enumerate(sub_files, 1):\n",
        "                    ffmpeg_cmd.extend([\n",
        "                        \"-map\", f\"{idx}:0\",\n",
        "                        f\"-metadata:s:s:{idx-1}\", f\"language={lang[:3].lower()}\",\n",
        "                        f\"-metadata:s:s:{idx-1}\", f\"title={lang}\"\n",
        "                    ])\n",
        "\n",
        "                # Output settings\n",
        "                ffmpeg_cmd.extend([\n",
        "                    \"-c:v\", \"copy\",\n",
        "                    \"-c:a\", \"copy\",\n",
        "                    \"-c:s\", \"mov_text\",  # MP4-compatible subtitle codec\n",
        "                    \"-y\",\n",
        "                    output_file\n",
        "                ])\n",
        "\n",
        "                result = subprocess.run(ffmpeg_cmd, capture_output=True, text=True)\n",
        "\n",
        "                # Cleanup\n",
        "                try:\n",
        "                    os.remove(temp_video)\n",
        "                    for sub_file, _ in sub_files:\n",
        "                        os.remove(sub_file)\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "                if result.returncode == 0 and os.path.exists(output_file):\n",
        "                    log(\"INFO\", f\"âœ… Download complete with subtitles: {os.path.basename(output_file)}\")\n",
        "                    return True\n",
        "                else:\n",
        "                    log(\"WARN\", \"Subtitle embedding failed, keeping video only\")\n",
        "                    if os.path.exists(temp_video):\n",
        "                        shutil.move(temp_video, output_file)\n",
        "                    return os.path.exists(output_file)\n",
        "            else:\n",
        "                # No subtitles downloaded, just rename temp file\n",
        "                shutil.move(temp_video, output_file)\n",
        "                return True\n",
        "\n",
        "        else:\n",
        "            # No subtitles, standard download\n",
        "            proc = subprocess.Popen(\n",
        "                cmd,\n",
        "                stdout=subprocess.PIPE,\n",
        "                stderr=subprocess.STDOUT,\n",
        "                universal_newlines=True,\n",
        "                bufsize=1,\n",
        "            )\n",
        "            with tqdm(\n",
        "                total=100,\n",
        "                unit=\"%\",\n",
        "                desc=f\"Ep {episode_label}\",\n",
        "                bar_format=\"{desc}: {percentage:3.0f}%|{bar}| {elapsed}\",\n",
        "            ) as pbar:\n",
        "                last = 0.0\n",
        "                for line in proc.stdout:\n",
        "                    m = re.search(r\"\\[download\\]\\s+(\\d+\\.?\\d*)%\", line)\n",
        "                    if m:\n",
        "                        cur = float(m.group(1))\n",
        "                        delta = cur - last\n",
        "                        if delta > 0:\n",
        "                            pbar.update(delta)\n",
        "                            last = cur\n",
        "            proc.wait()\n",
        "            if proc.returncode == 0 and os.path.exists(output_file):\n",
        "                log(\"INFO\", f\"âœ… Download complete: {os.path.basename(output_file)}\")\n",
        "                return True\n",
        "            log(\"ERROR\", f\"yt-dlp failed with code {proc.returncode}\")\n",
        "            return False\n",
        "\n",
        "    except Exception as e:\n",
        "        log(\"ERROR\", f\"yt-dlp error: {e}\")\n",
        "        return False\n",
        "\n",
        "def download_direct(url: str, output_file: str, episode_label: str) -> bool:\n",
        "    log(\"INFO\", f\"Using direct HTTP -> {os.path.basename(output_file)}\")\n",
        "    try:\n",
        "        with scraper.get(url, headers=HEADERS, stream=True, timeout=RETRY_CONFIG[\"timeout\"]) as r:\n",
        "            r.raise_for_status()\n",
        "            total = int(r.headers.get(\"Content-Length\", 0))\n",
        "            os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "            with open(output_file, \"wb\") as f:\n",
        "                if total > 0:\n",
        "                    with tqdm(total=total, unit=\"B\", unit_scale=True, desc=f\"Ep {episode_label}\") as pbar:\n",
        "                        for chunk in r.iter_content(chunk_size=8192):\n",
        "                            if not chunk:\n",
        "                                continue\n",
        "                            f.write(chunk)\n",
        "                            pbar.update(len(chunk))\n",
        "                else:\n",
        "                    for chunk in r.iter_content(chunk_size=8192):\n",
        "                        if chunk:\n",
        "                            f.write(chunk)\n",
        "        log(\"INFO\", f\"âœ… Download complete: {os.path.basename(output_file)}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        log(\"ERROR\", f\"Direct download error: {e}\")\n",
        "        return False\n",
        "\n",
        "def is_m3u8_url(url: str) -> bool:\n",
        "    if \".m3u8\" in url.split(\"?\")[0]:\n",
        "        return True\n",
        "    try:\n",
        "        r = scraper.head(url, headers=HEADERS, timeout=10)\n",
        "        ctype = r.headers.get(\"Content-Type\", \"\")\n",
        "        return \"application/vnd.apple.mpegurl\" in ctype or \"application/x-mpegURL\" in ctype\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def download_episode(video_data: Dict[str, Any], output_file: str, episode_label: str) -> bool:\n",
        "    \"\"\"Download episode with subtitle support.\"\"\"\n",
        "    url = video_data[\"video_url\"]\n",
        "    subtitles = video_data.get(\"subtitles\", [])\n",
        "\n",
        "    use_m3u8 = is_m3u8_url(url)\n",
        "    if download_method == \"yt-dlp\":\n",
        "        primary = lambda u, o: download_with_ytdlp(u, o, episode_label, subtitles)\n",
        "        fallback = lambda u, o: download_direct(u, o, episode_label)\n",
        "    else:\n",
        "        primary = lambda u, o: download_direct(u, o, episode_label)\n",
        "        fallback = lambda u, o: download_direct(u, o, episode_label)\n",
        "\n",
        "    for attempt in range(1, RETRY_CONFIG[\"max_retries\"] + 1):\n",
        "        if os.path.exists(output_file):\n",
        "            os.remove(output_file)\n",
        "        if attempt > 1:\n",
        "            log(\"INFO\", f\"Retry {attempt}/{RETRY_CONFIG['max_retries']} for {os.path.basename(output_file)}\")\n",
        "        if primary(url, output_file):\n",
        "            return True\n",
        "        time.sleep(RETRY_CONFIG[\"sleep_between\"])\n",
        "        if attempt == RETRY_CONFIG[\"max_retries\"] - 1:\n",
        "            log(\"WARN\", \"Primary method failing; trying fallback.\")\n",
        "            if fallback(url, output_file):\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "# ================================================================\n",
        "# Merge\n",
        "# ================================================================\n",
        "\n",
        "def merge_videos(\n",
        "    file_list: List[str],\n",
        "    anime_title: str,\n",
        "    season_num: int,\n",
        "    first_ep_id: str,\n",
        "    last_ep_id: str,\n",
        ") -> Optional[str]:\n",
        "    if not file_list:\n",
        "        log(\"ERROR\", \"No files to merge.\")\n",
        "        return None\n",
        "\n",
        "    valid_files = [f for f in file_list if os.path.exists(f)]\n",
        "    if len(valid_files) != len(file_list):\n",
        "        log(\"ERROR\", \"Some input files for merging are missing.\")\n",
        "        return None\n",
        "\n",
        "    merged_filename = f\"{anime_title} Season {season_num:02d} Episodes {first_ep_id}-{last_ep_id}.mp4\"\n",
        "    merged_filename = re.sub(r'[<>:\"/\\\\|?*]', \"\", merged_filename)\n",
        "    merged_path = os.path.join(os.path.dirname(file_list[0]), merged_filename)\n",
        "\n",
        "    log(\"INFO\", f\"Merging {len(valid_files)} files into {merged_filename}\")\n",
        "\n",
        "    list_file = os.path.join(os.path.dirname(file_list[0]), \"filelist_merge.txt\")\n",
        "    try:\n",
        "        with open(list_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            for vf in valid_files:\n",
        "                f.write(f\"file '{os.path.abspath(vf)}'\\n\")\n",
        "\n",
        "        cmd = [\n",
        "            \"ffmpeg\",\n",
        "            \"-f\", \"concat\",\n",
        "            \"-safe\", \"0\",\n",
        "            \"-i\", list_file,\n",
        "            \"-c:v\", \"copy\",\n",
        "            \"-c:a\", \"copy\",\n",
        "            \"-c:s\", \"copy\",  # Also copy subtitle streams\n",
        "            \"-y\",\n",
        "            \"-loglevel\", \"info\",\n",
        "            merged_path,\n",
        "        ]\n",
        "        proc = subprocess.Popen(\n",
        "            cmd,\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE,\n",
        "            universal_newlines=True,\n",
        "        )\n",
        "\n",
        "        print(\"Merging...\", end=\"\", flush=True)\n",
        "        ffmpeg_err = []\n",
        "        for line in proc.stderr:\n",
        "            ffmpeg_err.append(line.rstrip())\n",
        "            if \"time=\" in line:\n",
        "                print(\".\", end=\"\", flush=True)\n",
        "        proc.wait()\n",
        "        print()\n",
        "\n",
        "        if proc.returncode != 0 or not os.path.exists(merged_path):\n",
        "            log(\"ERROR\", \"ffmpeg merge failed. Last lines:\")\n",
        "            for l in ffmpeg_err[-10:]:\n",
        "                print(l)\n",
        "            return None\n",
        "\n",
        "        log(\"INFO\", f\"âœ… Merged: {merged_filename}\")\n",
        "        return merged_path\n",
        "    except Exception as e:\n",
        "        log(\"ERROR\", f\"Merge error: {e}\")\n",
        "        return None\n",
        "    finally:\n",
        "        try:\n",
        "            if os.path.exists(list_file):\n",
        "                os.remove(list_file)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "# ================================================================\n",
        "# Upload helpers\n",
        "# ================================================================\n",
        "\n",
        "def upload_to_gofile(filepath: str) -> Optional[str]:\n",
        "    try:\n",
        "        filename = os.path.basename(filepath)\n",
        "        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
        "        log(\"INFO\", f\"Uploading to GoFile: {filename} ({size_mb:.2f} MB)\")\n",
        "\n",
        "        server = None\n",
        "        for attempt in range(1, 6):\n",
        "            try:\n",
        "                r = requests.get(\"https://api.gofile.io/servers\", timeout=15)\n",
        "                data = r.json()\n",
        "\n",
        "                if data['status'] == 'ok' and data['data']['servers']:\n",
        "                    server = data['data']['servers'][0]['name']\n",
        "                    break\n",
        "                else:\n",
        "                    log(\"WARN\", f\"GoFile busy (Attempt {attempt}/5)... Waiting 3s.\")\n",
        "            except Exception:\n",
        "                pass\n",
        "            time.sleep(3)\n",
        "\n",
        "        if not server:\n",
        "            log(\"ERROR\", \"âŒ GoFile upload failed: No servers available currently.\")\n",
        "            return None\n",
        "\n",
        "        upload_url = f\"https://{server}.gofile.io/contents/uploadfile\"\n",
        "        with open(filepath, \"rb\") as f:\n",
        "            with tqdm(total=size_mb, unit=\"MB\", desc=\"GoFile Upload\") as pbar:\n",
        "\n",
        "                class ProgressFile:\n",
        "                    def __init__(self, file_obj, pbar):\n",
        "                        self.f = file_obj\n",
        "                        self.pbar = pbar\n",
        "\n",
        "                    def read(self, size=-1):\n",
        "                        data = self.f.read(size)\n",
        "                        if not data: return data\n",
        "                        self.pbar.update(len(data) / (1024 * 1024))\n",
        "                        return data\n",
        "\n",
        "                    def __getattr__(self, name):\n",
        "                        return getattr(self.f, name)\n",
        "\n",
        "                pf = ProgressFile(f, pbar)\n",
        "                resp = requests.post(upload_url, files={\"file\": (filename, pf)}, timeout=7200)\n",
        "\n",
        "        j = resp.json()\n",
        "        if j.get(\"status\") == \"ok\":\n",
        "            link = j[\"data\"][\"downloadPage\"]\n",
        "            log(\"INFO\", f\"âœ… GoFile link: {link}\")\n",
        "            return link\n",
        "        else:\n",
        "            log(\"ERROR\", f\"GoFile upload failed: {j}\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        log(\"ERROR\", f\"GoFile upload error: {e}\")\n",
        "        return None\n",
        "\n",
        "def upload_to_gdrive(filepath: str) -> Optional[str]:\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "\n",
        "        if not os.path.exists(\"/content/drive/MyDrive\"):\n",
        "            log(\"INFO\", \"Mounting Google Drive...\")\n",
        "            drive.mount(\"/content/drive\")\n",
        "\n",
        "        dest_dir = \"/content/drive/MyDrive/AnimeKai_Downloads/\"\n",
        "        os.makedirs(dest_dir, exist_ok=True)\n",
        "        filename = os.path.basename(filepath)\n",
        "        dest_path = os.path.join(dest_dir, filename)\n",
        "\n",
        "        total = os.path.getsize(filepath)\n",
        "        log(\"INFO\", f\"Copying to GDrive: {filename}\")\n",
        "        with open(filepath, \"rb\") as src, open(dest_path, \"wb\") as dst:\n",
        "            with tqdm(total=total, unit=\"B\", unit_scale=True, desc=\"Uploading to GDrive\") as pbar:\n",
        "                while True:\n",
        "                    chunk = src.read(8192)\n",
        "                    if not chunk:\n",
        "                        break\n",
        "                    dst.write(chunk)\n",
        "                    pbar.update(len(chunk))\n",
        "\n",
        "        log(\"INFO\", f\"âœ… Uploaded to {dest_path}\")\n",
        "        return dest_path\n",
        "    except Exception as e:\n",
        "        log(\"ERROR\", f\"GDrive upload error: {e}\")\n",
        "        return None\n",
        "\n",
        "# ================================================================\n",
        "# Filename generator (NEW - FIXED)\n",
        "# ================================================================\n",
        "\n",
        "def generate_episode_filename(anime_title: str, season_num: int, ep_id: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate proper episode filename: Anime Name Season xx Episode xx.mp4\n",
        "    Example: My Dress Up Darling Season 02 Episode 01.mp4\n",
        "    \"\"\"\n",
        "    # Pad episode number if it's a simple integer\n",
        "    try:\n",
        "        ep_num = float(ep_id)\n",
        "        if ep_num == int(ep_num):\n",
        "            # Simple integer episode (1, 2, 3...)\n",
        "            ep_formatted = f\"{int(ep_num):02d}\"\n",
        "        else:\n",
        "            # Decimal episode (12.5, etc.)\n",
        "            ep_formatted = ep_id\n",
        "    except ValueError:\n",
        "        # Special episodes (SP, OVA, etc.)\n",
        "        ep_formatted = ep_id\n",
        "\n",
        "    filename = f\"{anime_title} Season {season_num:02d} Episode {ep_formatted}.mp4\"\n",
        "    # Remove invalid characters\n",
        "    filename = re.sub(r'[<>:\"/\\\\|?*]', \"\", filename)\n",
        "    return filename\n",
        "\n",
        "# ================================================================\n",
        "# main() - FIXED for proper episode naming\n",
        "# ================================================================\n",
        "\n",
        "def parse_episode_id(ep_id: str) -> Tuple[int, float]:\n",
        "    return safe_episode_key(ep_id)\n",
        "\n",
        "def in_episode_range(ep_id: str, start_id: str, end_id: str) -> bool:\n",
        "    s_key = parse_episode_id(start_id)\n",
        "    e_key = parse_episode_id(end_id)\n",
        "    k = parse_episode_id(ep_id)\n",
        "    return s_key <= k <= e_key\n",
        "\n",
        "def main():\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"ðŸŽ¬ ANIMEKAI EPISODE DOWNLOADER & MERGER (FIXED)\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    log(\"INFO\", f\"Processing: {anime_url}\")\n",
        "\n",
        "    anime_id, anime_title = get_anime_details(anime_url)\n",
        "    if not anime_id:\n",
        "        raise RuntimeError(\"Could not extract anime ID â€“ check URL or site changes.\")\n",
        "\n",
        "    log(\"INFO\", f\"Anime ID: {anime_id}\")\n",
        "    log(\"INFO\", f\"Title: {anime_title}\")\n",
        "\n",
        "    detected_season = detect_season_from_title(anime_title)\n",
        "    final_season = season_number if season_number > 0 else detected_season\n",
        "    log(\"INFO\", f\"Season: {final_season} ({'auto' if season_number == 0 else 'manual'})\")\n",
        "\n",
        "    episodes = get_episode_list(anime_id)\n",
        "    if not episodes:\n",
        "        raise RuntimeError(\"No episodes found.\")\n",
        "\n",
        "    log(\"INFO\", f\"Found {len(episodes)} episodes\")\n",
        "\n",
        "    if download_mode == \"Single Episode\":\n",
        "        target = single_episode.strip()\n",
        "        selected = [ep for ep in episodes if ep[\"id\"] == target]\n",
        "    elif download_mode == \"Episode Range\":\n",
        "        if parse_episode_id(start_episode) > parse_episode_id(end_episode):\n",
        "            raise ValueError(f\"Invalid episode range: {start_episode} > {end_episode}\")\n",
        "        selected = [ep for ep in episodes if in_episode_range(ep[\"id\"], start_episode, end_episode)]\n",
        "    else:\n",
        "        selected = episodes\n",
        "\n",
        "    if not selected:\n",
        "        raise RuntimeError(\"No episodes match your selection.\")\n",
        "\n",
        "    log(\"INFO\", f\"Will download {len(selected)} episode(s)\")\n",
        "\n",
        "    download_dir = os.path.join(\"downloads\", anime_title)\n",
        "    os.makedirs(download_dir, exist_ok=True)\n",
        "    log(\"INFO\", f\"Download directory: {download_dir}\")\n",
        "\n",
        "    downloaded_files: List[str] = []\n",
        "    failed_episodes: List[str] = []\n",
        "\n",
        "    for idx, ep in enumerate(selected, 1):\n",
        "        ep_id = ep[\"id\"]\n",
        "        print(\"\\n\" + \"-\" * 50)\n",
        "        log(\"INFO\", f\"[{idx}/{len(selected)}] Episode {ep_id}\")\n",
        "\n",
        "        servers = get_video_servers(ep[\"token\"])\n",
        "        if not servers:\n",
        "            log(\"ERROR\", \"No servers available for this episode.\")\n",
        "            failed_episodes.append(ep_id)\n",
        "            continue\n",
        "\n",
        "        server = choose_server(servers, prefer_type, prefer_server)\n",
        "        if not server:\n",
        "            log(\"ERROR\", \"Could not choose any server.\")\n",
        "            failed_episodes.append(ep_id)\n",
        "            continue\n",
        "\n",
        "        log(\"INFO\", f\"Using server: {server['server_name']} (type={server['type']})\")\n",
        "\n",
        "        # Get video data with subtitles\n",
        "        video_data = get_video_data(server[\"server_id\"])\n",
        "        if not video_data:\n",
        "            log(\"ERROR\", \"Could not resolve video data.\")\n",
        "            failed_episodes.append(ep_id)\n",
        "            continue\n",
        "\n",
        "        # FIXED: Generate proper filename\n",
        "        filename = generate_episode_filename(anime_title, final_season, ep_id)\n",
        "        filepath = os.path.join(download_dir, filename)\n",
        "\n",
        "        log(\"INFO\", f\"Output filename: {filename}\")\n",
        "\n",
        "        if download_episode(video_data, filepath, ep_id):\n",
        "            downloaded_files.append(filepath)\n",
        "        else:\n",
        "            log(\"ERROR\", \"All download attempts failed for this episode.\")\n",
        "            failed_episodes.append(ep_id)\n",
        "        time.sleep(1)\n",
        "\n",
        "    merged_video = None\n",
        "    # Only merge if multiple episodes AND merge is enabled\n",
        "    if merge_episodes and len(downloaded_files) > 1:\n",
        "        # Create mapping with proper filenames\n",
        "        file_by_ep = {}\n",
        "        for ep in selected:\n",
        "            filename = generate_episode_filename(anime_title, final_season, ep[\"id\"])\n",
        "            filepath = os.path.join(download_dir, filename)\n",
        "            file_by_ep[ep[\"id\"]] = filepath\n",
        "\n",
        "        ordered_files = [file_by_ep[ep[\"id\"]] for ep in selected if os.path.exists(file_by_ep[ep[\"id\"]])]\n",
        "\n",
        "        first_ep_id = selected[0][\"id\"]\n",
        "        last_ep_id = selected[-1][\"id\"]\n",
        "        merged_video = merge_videos(ordered_files, anime_title, final_season, first_ep_id, last_ep_id)\n",
        "\n",
        "        if merged_video and not keep_individual_files:\n",
        "            log(\"INFO\", \"Removing individual episode files after merge.\")\n",
        "            for fpath in ordered_files:\n",
        "                try:\n",
        "                    os.remove(fpath)\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"ðŸ“Š DOWNLOAD SUMMARY\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"\\nâœ… Successfully downloaded: {len(downloaded_files)} episode(s)\")\n",
        "    if failed_episodes:\n",
        "        print(f\"âŒ Failed episodes: {', '.join(failed_episodes)}\")\n",
        "\n",
        "    if merged_video:\n",
        "        size_mb = os.path.getsize(merged_video) / (1024 * 1024)\n",
        "        print(f\"\\nðŸ”— Merged file: {os.path.basename(merged_video)} ({size_mb:.2f} MB)\")\n",
        "        if not keep_individual_files:\n",
        "            print(\"   Individual episode files were deleted.\")\n",
        "    elif downloaded_files:\n",
        "        total = sum(os.path.getsize(f) for f in downloaded_files if os.path.exists(f)) / (1024 * 1024)\n",
        "        print(f\"\\nðŸ’¾ Total size of downloaded episodes: {total:.2f} MB\")\n",
        "\n",
        "    print(f\"\\nðŸ“ Local files location: {download_dir}\")\n",
        "\n",
        "    # Show downloaded filenames\n",
        "    if downloaded_files:\n",
        "        print(\"\\nðŸ“ Downloaded files:\")\n",
        "        for fpath in downloaded_files:\n",
        "            if os.path.exists(fpath):\n",
        "                print(f\"   â€¢ {os.path.basename(fpath)}\")\n",
        "\n",
        "    # FIXED: Upload logic for single episodes\n",
        "    files_to_upload: List[str] = []\n",
        "\n",
        "    # Determine which files to upload\n",
        "    if len(downloaded_files) == 1:\n",
        "        # Single episode - always upload it\n",
        "        files_to_upload = downloaded_files\n",
        "        log(\"INFO\", \"Single episode detected - will upload individual file\")\n",
        "    elif merge_episodes and len(downloaded_files) > 1:\n",
        "        # Multiple episodes with merge enabled\n",
        "        if upload_merged_only and merged_video:\n",
        "            # Only upload merged file\n",
        "            files_to_upload = [merged_video]\n",
        "            log(\"INFO\", \"Upload merged only - will upload merged file\")\n",
        "        elif merged_video:\n",
        "            # Upload merged + individuals if kept\n",
        "            files_to_upload = [merged_video]\n",
        "            if keep_individual_files:\n",
        "                files_to_upload.extend([f for f in downloaded_files if os.path.exists(f)])\n",
        "            log(\"INFO\", f\"Will upload merged file + {len(files_to_upload)-1} individual files\")\n",
        "        else:\n",
        "            # Merge failed, upload individuals\n",
        "            files_to_upload = [f for f in downloaded_files if os.path.exists(f)]\n",
        "            log(\"INFO\", \"Merge failed - will upload individual files\")\n",
        "    else:\n",
        "        # Multiple episodes, merge disabled - upload all\n",
        "        files_to_upload = [f for f in downloaded_files if os.path.exists(f)]\n",
        "        log(\"INFO\", f\"Merge disabled - will upload {len(files_to_upload)} individual files\")\n",
        "\n",
        "    gofile_links = []\n",
        "    gdrive_paths = []\n",
        "\n",
        "    if files_to_upload and upload_destination != \"None (Keep Local)\":\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"ðŸ“¤ UPLOADING FILES\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"\\nFiles to upload: {len(files_to_upload)}\")\n",
        "\n",
        "        for path in files_to_upload:\n",
        "            if not os.path.exists(path):\n",
        "                log(\"WARN\", f\"File not found, skipping: {os.path.basename(path)}\")\n",
        "                continue\n",
        "\n",
        "            print(f\"\\nðŸ“¤ Processing: {os.path.basename(path)}\")\n",
        "\n",
        "            if upload_destination in [\"GoFile.io Only\", \"Both\"]:\n",
        "                link = upload_to_gofile(path)\n",
        "                if link:\n",
        "                    gofile_links.append((os.path.basename(path), link))\n",
        "\n",
        "            if upload_destination in [\"Google Drive Only\", \"Both\"]:\n",
        "                dpath = upload_to_gdrive(path)\n",
        "                if dpath:\n",
        "                    gdrive_paths.append(os.path.basename(dpath))\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"âœ… UPLOAD COMPLETE\")\n",
        "        print(\"=\" * 70)\n",
        "        if gofile_links:\n",
        "            print(\"\\nðŸ”— GoFile Links:\")\n",
        "            for name, link in gofile_links:\n",
        "                print(f\"   â€¢ {name}: {link}\")\n",
        "        if gdrive_paths:\n",
        "            print(\"\\nðŸ“ Google Drive files (MyDrive/AnimeKai_Downloads/):\")\n",
        "            for name in gdrive_paths:\n",
        "                print(f\"   â€¢ {name}\")\n",
        "    elif upload_destination == \"None (Keep Local)\":\n",
        "        print(\"\\nðŸ“ Upload disabled - files kept locally only\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"ðŸŽ‰ ALL DONE!\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"\\nðŸ“º Anime: {anime_title}\")\n",
        "    print(f\"ðŸ“Š Season: {final_season}\")\n",
        "    print(f\"ðŸ“¥ Downloaded: {len(downloaded_files)} episode(s)\")\n",
        "    if failed_episodes:\n",
        "        print(f\"âŒ Failed: {', '.join(failed_episodes)}\")\n",
        "    if merged_video:\n",
        "        print(f\"ðŸ”— Merged file: {os.path.basename(merged_video)}\")\n",
        "    print(f\"\\nðŸ“ Local files: {download_dir}\")\n",
        "\n",
        "    # Show subtitle info if Soft Sub or Dub was selected\n",
        "    if prefer_type in [\"Soft Sub\", \"Dub (with subs)\"]:\n",
        "        print(f\"\\nðŸ’¬ Subtitles: Embedded in video file(s) when available\")\n",
        "        print(\"   (Enable subtitles in your video player's subtitle menu)\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "\n",
        "# Run main\n",
        "try:\n",
        "    main()\n",
        "except Exception as e:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"âŒ ERROR\")\n",
        "    print(\"=\" * 70)\n",
        "    log(\"ERROR\", f\"Fatal error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    print(\"\\n\" + \"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7PZkspKUd_a"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ðŸŽ¬ Video Episode Merger & Uploader\n",
        "# Download ZIP file with video episodes, extract, merge them in order, and upload\n",
        "\n",
        "# @title ðŸ”§ **Install Dependencies** { display-mode: \"form\" }\n",
        "print(\"ðŸ“¦ Installing required packages...\")\n",
        "!pip install -q natsort requests\n",
        "!apt-get -qq install -y ffmpeg > /dev/null 2>&1\n",
        "print(\"âœ… All dependencies installed!\\n\")\n",
        "\n",
        "# @title âš™ï¸ **Configuration** { display-mode: \"form\" }\n",
        "\n",
        "#@markdown ### ðŸ“¥ Download Settings\n",
        "#@markdown Enter the direct download link (DDL) for your ZIP file:\n",
        "zip_url = \"https://example.com/videos.zip\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Custom User-Agent (leave default if unsure):\n",
        "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### ðŸŽ¬ Merge Settings\n",
        "#@markdown Custom output name (leave empty to auto-detect from episodes):\n",
        "custom_output_name = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Video quality for merge:\n",
        "merge_quality = \"Copy Original (Fastest)\" #@param [\"Copy Original (Fastest)\", \"Re-encode High Quality\", \"Re-encode Compressed\"]\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### ðŸ“¤ Upload Settings\n",
        "upload_destination = \"Both (GoFile + Google Drive)\" #@param [\"GoFile.io Only\", \"Google Drive Only\", \"Both (GoFile + Google Drive)\", \"None (Keep Local Only)\"]\n",
        "\n",
        "#@markdown Create ZIP of merged video for upload?\n",
        "create_upload_zip = False #@param {type:\"boolean\"}\n",
        "\n",
        "print(\"âœ… Configuration set!\")\n",
        "\n",
        "# @title ðŸ“¥ **Download ZIP File** { display-mode: \"form\" }\n",
        "import requests\n",
        "import os\n",
        "import re\n",
        "from urllib.parse import unquote, urlparse\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"ðŸ”½ Starting download...\")\n",
        "print(f\"ðŸ”— URL: {zip_url[:60]}...\" if len(zip_url) > 60 else f\"ðŸ”— URL: {zip_url}\")\n",
        "\n",
        "headers = {'User-Agent': user_agent}\n",
        "\n",
        "try:\n",
        "    response = requests.get(zip_url, headers=headers, stream=True, allow_redirects=True)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    # Smart filename detection\n",
        "    zip_filename = None\n",
        "\n",
        "    # Method 1: Content-Disposition header\n",
        "    if 'Content-Disposition' in response.headers:\n",
        "        cd = response.headers['Content-Disposition']\n",
        "        filenames = re.findall(r'filename\\*?=[\"\\']?(?:UTF-8\\'\\')?([^\"\\';]+)[\"\\']?', cd)\n",
        "        if filenames:\n",
        "            zip_filename = unquote(filenames[0])\n",
        "            print(f\"ðŸ“‹ Filename from header: {zip_filename}\")\n",
        "\n",
        "    # Method 2: Final URL after redirects\n",
        "    if not zip_filename:\n",
        "        final_url = response.url\n",
        "        url_path = urlparse(final_url).path\n",
        "        zip_filename = os.path.basename(url_path)\n",
        "        zip_filename = unquote(zip_filename)\n",
        "        print(f\"ðŸ“‹ Filename from URL: {zip_filename}\")\n",
        "\n",
        "    # Method 3: Extract meaningful name from URL\n",
        "    if not zip_filename or zip_filename in ['', 'download', 'file']:\n",
        "        # Try to extract from full URL path\n",
        "        url_parts = [p for p in urlparse(zip_url).path.split('/') if p and p != 'download']\n",
        "        if url_parts:\n",
        "            zip_filename = url_parts[-1]\n",
        "            zip_filename = unquote(zip_filename)\n",
        "\n",
        "    # Ensure .zip extension\n",
        "    if not zip_filename.lower().endswith('.zip'):\n",
        "        if '.' not in zip_filename:\n",
        "            zip_filename += '.zip'\n",
        "        else:\n",
        "            zip_filename = os.path.splitext(zip_filename)[0] + '.zip'\n",
        "\n",
        "    # Clean filename (remove invalid characters)\n",
        "    zip_filename = re.sub(r'[<>:\"|?*\\\\]', '_', zip_filename)\n",
        "    zip_filename = re.sub(r'[\\x00-\\x1f]', '', zip_filename)  # Remove control characters\n",
        "\n",
        "    print(f\"ðŸ’¾ Saving as: {zip_filename}\")\n",
        "\n",
        "    total_size = int(response.headers.get('content-length', 0))\n",
        "    downloaded = 0\n",
        "\n",
        "    with open(zip_filename, 'wb') as f:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            if chunk:\n",
        "                f.write(chunk)\n",
        "                downloaded += len(chunk)\n",
        "                if total_size:\n",
        "                    percent = (downloaded / total_size) * 100\n",
        "                    mb_downloaded = downloaded / (1024*1024)\n",
        "                    mb_total = total_size / (1024*1024)\n",
        "                    print(f\"\\râ³ Progress: {percent:.1f}% ({mb_downloaded:.1f}/{mb_total:.1f} MB)\", end='')\n",
        "\n",
        "    print(f\"\\nâœ… Downloaded: {zip_filename} ({downloaded / (1024*1024):.2f} MB)\")\n",
        "\n",
        "    # Extract base name for later use\n",
        "    ZIP_BASE_NAME = os.path.splitext(zip_filename)[0]\n",
        "    ZIP_BASE_NAME = re.sub(r'[_\\-\\s]+', ' ', ZIP_BASE_NAME).strip()\n",
        "\n",
        "    print(f\"ðŸ“¦ Base name extracted: '{ZIP_BASE_NAME}'\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nâŒ Download failed: {str(e)}\")\n",
        "    import traceback\n",
        "    print(traceback.format_exc())\n",
        "    raise\n",
        "\n",
        "# @title ðŸ“¦ **Extract ZIP File** { display-mode: \"form\" }\n",
        "import zipfile\n",
        "\n",
        "extract_folder = \"extracted_videos\"\n",
        "os.makedirs(extract_folder, exist_ok=True)\n",
        "\n",
        "print(f\"\\nðŸ“‚ Extracting to: {extract_folder}/\")\n",
        "print(\"â³ Please wait...\")\n",
        "\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "        file_list = zip_ref.namelist()\n",
        "        total_files = len(file_list)\n",
        "\n",
        "        print(f\"ðŸ“‹ Found {total_files} file(s) in ZIP\\n\")\n",
        "\n",
        "        # Extract with progress\n",
        "        for idx, file in enumerate(file_list, 1):\n",
        "            zip_ref.extract(file, extract_folder)\n",
        "            if idx % 5 == 0 or idx == total_files:\n",
        "                print(f\"\\râ³ Extracting: {idx}/{total_files} files...\", end='')\n",
        "\n",
        "        print(f\"\\n\\nðŸ“„ Extracted files:\")\n",
        "        video_count = 0\n",
        "        for file in file_list:\n",
        "            file_lower = file.lower()\n",
        "            is_video = any(file_lower.endswith(ext) for ext in ['.mp4', '.mkv', '.avi', '.mov', '.flv', '.wmv', '.webm', '.m4v'])\n",
        "            icon = \"ðŸŽ¬\" if is_video else \"ðŸ“„\"\n",
        "            print(f\"  {icon} {file}\")\n",
        "            if is_video:\n",
        "                video_count += 1\n",
        "\n",
        "        print(f\"\\nâœ… Extraction complete! Found {video_count} video file(s)\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nâŒ Extraction failed: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# @title ðŸ” **Detect and Sort Episodes** { display-mode: \"form\" }\n",
        "import re\n",
        "from natsort import natsorted\n",
        "\n",
        "def extract_episode_info(filename):\n",
        "    \"\"\"Enhanced episode detection with better pattern matching\"\"\"\n",
        "    name = os.path.basename(filename)\n",
        "\n",
        "    # Combined season and episode patterns (S01E01, S1E1, etc.)\n",
        "    combined_patterns = [\n",
        "        r'[Ss](\\d+)[Ee](\\d+)',  # S01E01, S1E1\n",
        "        r'[Ss]eason[\\s._-]*(\\d+)[\\s._-]*[Ee]pisode[\\s._-]*(\\d+)',  # Season 1 Episode 1\n",
        "        r'[Ss]eason[\\s._-]*(\\d+)[\\s._-]*[Ee][Pp][\\s._-]*(\\d+)',  # Season 1 Ep 1\n",
        "        r'(\\d+)[xX](\\d+)',  # 1x01\n",
        "    ]\n",
        "\n",
        "    # Try combined patterns first\n",
        "    for pattern in combined_patterns:\n",
        "        match = re.search(pattern, name, re.IGNORECASE)\n",
        "        if match:\n",
        "            return int(match.group(1)), int(match.group(2))\n",
        "\n",
        "    # Separate season patterns\n",
        "    season_patterns = [\n",
        "        r'[Ss]eason[\\s._-]*(\\d+)',\n",
        "        r'[Ss](\\d+)(?![Ee])',  # S1 but not followed by E\n",
        "        r'Season[\\s._-]*(\\d+)',\n",
        "    ]\n",
        "\n",
        "    # Episode patterns\n",
        "    episode_patterns = [\n",
        "        r'[Ee]pisode[\\s._-]*(\\d+)',\n",
        "        r'[Ee][Pp][\\s._-]*(\\d+)',\n",
        "        r'[Ee](\\d+)',\n",
        "        r'Episode[\\s._-]*(\\d+)',\n",
        "        r'[\\s._-](\\d{1,3})[\\s._-]',  # Number surrounded by separators\n",
        "        r'^(\\d{1,3})[\\s._-]',  # Number at start\n",
        "        r'[\\s._-](\\d{1,3})\\.',  # Number before extension\n",
        "    ]\n",
        "\n",
        "    season = None\n",
        "    episode = None\n",
        "\n",
        "    # Find season\n",
        "    for pattern in season_patterns:\n",
        "        match = re.search(pattern, name, re.IGNORECASE)\n",
        "        if match:\n",
        "            season = int(match.group(1))\n",
        "            break\n",
        "\n",
        "    # Find episode\n",
        "    for pattern in episode_patterns:\n",
        "        match = re.search(pattern, name, re.IGNORECASE)\n",
        "        if match:\n",
        "            ep_num = int(match.group(1))\n",
        "            # Reasonable episode number (1-999)\n",
        "            if 1 <= ep_num <= 999:\n",
        "                episode = ep_num\n",
        "                break\n",
        "\n",
        "    return season, episode\n",
        "\n",
        "# Find all video files\n",
        "video_extensions = ['.mp4', '.mkv', '.avi', '.mov', '.flv', '.wmv', '.webm', '.m4v', '.ts', '.m2ts']\n",
        "video_files = []\n",
        "\n",
        "for root, dirs, files in os.walk(extract_folder):\n",
        "    for file in files:\n",
        "        if any(file.lower().endswith(ext) for ext in video_extensions):\n",
        "            full_path = os.path.join(root, file)\n",
        "            video_files.append(full_path)\n",
        "\n",
        "if not video_files:\n",
        "    print(\"âŒ No video files found in the ZIP!\")\n",
        "    raise Exception(\"No video files detected\")\n",
        "\n",
        "print(f\"ðŸŽ¬ Found {len(video_files)} video file(s)\\n\")\n",
        "\n",
        "# Extract info and sort\n",
        "video_info = []\n",
        "for vf in video_files:\n",
        "    season, episode = extract_episode_info(vf)\n",
        "    video_info.append({\n",
        "        'path': vf,\n",
        "        'name': os.path.basename(vf),\n",
        "        'season': season if season else 0,\n",
        "        'episode': episode if episode else 0\n",
        "    })\n",
        "\n",
        "# Sort by season, then episode, then natural name\n",
        "video_info.sort(key=lambda x: (x['season'], x['episode'], x['name']))\n",
        "\n",
        "print(\"ðŸ“‹ **Detected Episode Order:**\")\n",
        "print(\"=\" * 70)\n",
        "for idx, info in enumerate(video_info, 1):\n",
        "    s_info = f\"S{info['season']:02d}\" if info['season'] else \"S??\"\n",
        "    e_info = f\"E{info['episode']:02d}\" if info['episode'] else \"E??\"\n",
        "    size_mb = os.path.getsize(info['path']) / (1024*1024)\n",
        "    print(f\"{idx:2d}. [{s_info}{e_info}] {info['name'][:45]:<45} ({size_mb:.1f} MB)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# @title ðŸŽžï¸ **Merge Videos** { display-mode: \"form\" }\n",
        "import subprocess\n",
        "\n",
        "print(\"\\nðŸŽ¬ Preparing to merge videos...\")\n",
        "\n",
        "# Create file list for ffmpeg\n",
        "list_file = \"filelist.txt\"\n",
        "with open(list_file, 'w', encoding='utf-8') as f:\n",
        "    for info in video_info:\n",
        "        # Escape single quotes for ffmpeg\n",
        "        safe_path = info['path'].replace(\"'\", \"'\\\\''\")\n",
        "        f.write(f\"file '{safe_path}'\\n\")\n",
        "\n",
        "print(f\"âœ… Created merge list with {len(video_info)} video(s)\")\n",
        "\n",
        "# Determine output filename\n",
        "if custom_output_name:\n",
        "    output_name = custom_output_name\n",
        "    if not output_name.lower().endswith('.mp4'):\n",
        "        output_name += '.mp4'\n",
        "else:\n",
        "    # Auto-generate name\n",
        "    seasons = [v['season'] for v in video_info if v['season'] > 0]\n",
        "    episodes = [v['episode'] for v in video_info if v['episode'] > 0]\n",
        "\n",
        "    base_name = ZIP_BASE_NAME\n",
        "\n",
        "    if seasons and episodes:\n",
        "        min_season = min(seasons)\n",
        "        max_season = max(seasons)\n",
        "        min_episode = min(episodes)\n",
        "        max_episode = max(episodes)\n",
        "\n",
        "        if min_season == max_season:\n",
        "            output_name = f\"{base_name} Season {min_season:02d} Episodes {min_episode:02d}-{max_episode:02d}.mp4\"\n",
        "        else:\n",
        "            output_name = f\"{base_name} S{min_season:02d}-S{max_season:02d} Ep{min_episode:02d}-{max_episode:02d}.mp4\"\n",
        "    else:\n",
        "        output_name = f\"{base_name} Merged Complete.mp4\"\n",
        "\n",
        "# Clean output name\n",
        "output_name = re.sub(r'[<>:\"|?*\\\\]', '_', output_name)\n",
        "output_name = re.sub(r'\\s+', ' ', output_name).strip()\n",
        "\n",
        "print(f\"\\nðŸ“ Output filename: {output_name}\")\n",
        "\n",
        "# Build ffmpeg command based on quality setting\n",
        "if merge_quality == \"Copy Original (Fastest)\":\n",
        "    cmd = [\n",
        "        'ffmpeg', '-f', 'concat', '-safe', '0', '-i', list_file,\n",
        "        '-c', 'copy', output_name, '-y'\n",
        "    ]\n",
        "    print(\"âš¡ Mode: Fast merge (copy streams, no re-encoding)\")\n",
        "elif merge_quality == \"Re-encode High Quality\":\n",
        "    cmd = [\n",
        "        'ffmpeg', '-f', 'concat', '-safe', '0', '-i', list_file,\n",
        "        '-c:v', 'libx264', '-crf', '18', '-preset', 'slow',\n",
        "        '-c:a', 'aac', '-b:a', '192k',\n",
        "        output_name, '-y'\n",
        "    ]\n",
        "    print(\"ðŸŽ¨ Mode: High quality re-encode (slower, best quality)\")\n",
        "else:  # Compressed\n",
        "    cmd = [\n",
        "        'ffmpeg', '-f', 'concat', '-safe', '0', '-i', list_file,\n",
        "        '-c:v', 'libx264', '-crf', '23', '-preset', 'medium',\n",
        "        '-c:a', 'aac', '-b:a', '128k',\n",
        "        output_name, '-y'\n",
        "    ]\n",
        "    print(\"ðŸ“¦ Mode: Compressed re-encode (smaller file size)\")\n",
        "\n",
        "print(\"\\nâ³ Merging videos... This may take a while.\\n\")\n",
        "\n",
        "try:\n",
        "    # Run ffmpeg\n",
        "    process = subprocess.Popen(cmd, stderr=subprocess.PIPE, universal_newlines=True)\n",
        "\n",
        "    # Parse ffmpeg output for progress\n",
        "    duration_pattern = re.compile(r'Duration: (\\d{2}):(\\d{2}):(\\d{2})')\n",
        "    time_pattern = re.compile(r'time=(\\d{2}):(\\d{2}):(\\d{2})')\n",
        "\n",
        "    total_duration = None\n",
        "\n",
        "    for line in process.stderr:\n",
        "        # Get total duration\n",
        "        if total_duration is None:\n",
        "            dur_match = duration_pattern.search(line)\n",
        "            if dur_match:\n",
        "                h, m, s = map(int, dur_match.groups())\n",
        "                total_duration = h * 3600 + m * 60 + s\n",
        "\n",
        "        # Get current time\n",
        "        time_match = time_pattern.search(line)\n",
        "        if time_match and total_duration:\n",
        "            h, m, s = map(int, time_match.groups())\n",
        "            current_time = h * 3600 + m * 60 + s\n",
        "            percent = (current_time / total_duration) * 100\n",
        "            print(f\"\\rðŸŽ¬ Progress: {percent:.1f}% ({current_time//60}:{current_time%60:02d} / {total_duration//60}:{total_duration%60:02d})\", end='')\n",
        "\n",
        "    process.wait()\n",
        "\n",
        "    if process.returncode == 0:\n",
        "        file_size = os.path.getsize(output_name) / (1024*1024)\n",
        "        print(f\"\\n\\nâœ… **Merge Complete!**\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"ðŸ“ Output: {output_name}\")\n",
        "        print(f\"ðŸ’¾ Size: {file_size:.2f} MB\")\n",
        "        print(f\"ðŸŽ¬ Episodes: {len(video_info)}\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        MERGED_VIDEO = output_name\n",
        "    else:\n",
        "        print(f\"\\nâŒ Merge failed with exit code {process.returncode}\")\n",
        "        raise Exception(\"FFmpeg merge failed\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nâŒ Error during merge: {str(e)}\")\n",
        "    raise\n",
        "finally:\n",
        "    # Cleanup\n",
        "    if os.path.exists(list_file):\n",
        "        os.remove(list_file)\n",
        "\n",
        "# @title ðŸ“¦ **Create ZIP of Merged Video (Optional)** { display-mode: \"form\" }\n",
        "\n",
        "if create_upload_zip:\n",
        "    print(\"\\nðŸ“¦ Creating ZIP file of merged video...\")\n",
        "\n",
        "    zip_output = output_name.replace('.mp4', '.zip')\n",
        "\n",
        "    import zipfile\n",
        "    with zipfile.ZipFile(zip_output, 'w', zipfile.ZIP_DEFLATED, compresslevel=0) as zipf:\n",
        "        print(f\"â³ Adding {output_name} to ZIP...\")\n",
        "        zipf.write(output_name, os.path.basename(output_name))\n",
        "\n",
        "    zip_size = os.path.getsize(zip_output) / (1024*1024)\n",
        "    print(f\"âœ… ZIP created: {zip_output} ({zip_size:.2f} MB)\")\n",
        "\n",
        "    UPLOAD_FILE = zip_output\n",
        "else:\n",
        "    UPLOAD_FILE = MERGED_VIDEO\n",
        "    print(\"\\nðŸ“„ Will upload video file directly (no ZIP)\")\n",
        "\n",
        "# @title ðŸ“¤ **Upload Files** { display-mode: \"form\" }\n",
        "\n",
        "def upload_to_gofile(filepath):\n",
        "    \"\"\"Upload file to GoFile.io\"\"\"\n",
        "    try:\n",
        "        print(\"\\nðŸŒ GoFile.io Upload\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Get best server\n",
        "        server_response = requests.get('https://api.gofile.io/servers', timeout=30)\n",
        "        server_response.raise_for_status()\n",
        "        server_data = server_response.json()\n",
        "\n",
        "        if server_data['status'] != 'ok':\n",
        "            print(\"âŒ Failed to get GoFile server\")\n",
        "            return None\n",
        "\n",
        "        server = server_data['data']['servers'][0]['name']\n",
        "        print(f\"ðŸ“¡ Server: {server}\")\n",
        "\n",
        "        # Updated endpoint\n",
        "        upload_url = f'https://{server}.gofile.io/contents/uploadfile'\n",
        "\n",
        "        file_size_mb = os.path.getsize(filepath) / (1024*1024)\n",
        "        print(f\"ðŸ“¦ File: {os.path.basename(filepath)} ({file_size_mb:.2f} MB)\")\n",
        "        print(\"â³ Uploading... (this may take several minutes for large files)\")\n",
        "\n",
        "        with open(filepath, 'rb') as f:\n",
        "            files_data = {'file': (os.path.basename(filepath), f, 'application/octet-stream')}\n",
        "            response = requests.post(upload_url, files=files_data, timeout=7200)\n",
        "\n",
        "        response.raise_for_status()\n",
        "\n",
        "        result = response.json()\n",
        "        if result['status'] == 'ok':\n",
        "            download_page = result['data']['downloadPage']\n",
        "            print(\"âœ… Upload successful!\")\n",
        "            print(f\"ðŸ”— Link: {download_page}\")\n",
        "            return download_page\n",
        "        else:\n",
        "            print(f\"âŒ Upload failed: {result.get('message', 'Unknown error')}\")\n",
        "            return None\n",
        "\n",
        "    except requests.exceptions.Timeout:\n",
        "        print(\"âŒ Upload timed out - file may be too large for GoFile\")\n",
        "        return None\n",
        "    except requests.exceptions.JSONDecodeError:\n",
        "        print(\"âŒ Invalid response from GoFile - service may be down\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ GoFile error: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def upload_to_gdrive(filepath):\n",
        "    \"\"\"Upload file to Google Drive\"\"\"\n",
        "    try:\n",
        "        print(\"\\nâ˜ï¸ Google Drive Upload\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        from google.colab import drive\n",
        "\n",
        "        # Check if already mounted\n",
        "        if not os.path.exists('/content/drive/MyDrive'):\n",
        "            drive.mount('/content/drive', force_remount=False)\n",
        "            print(\"âœ… Google Drive mounted!\")\n",
        "        else:\n",
        "            print(\"âœ… Google Drive already mounted!\")\n",
        "\n",
        "        destination = '/content/drive/MyDrive/Merged_Videos/'\n",
        "        os.makedirs(destination, exist_ok=True)\n",
        "\n",
        "        dest_path = os.path.join(destination, os.path.basename(filepath))\n",
        "\n",
        "        # Check if file exists\n",
        "        if not os.path.exists(filepath):\n",
        "            print(f\"âŒ Source file not found: {filepath}\")\n",
        "            return None\n",
        "\n",
        "        file_size_mb = os.path.getsize(filepath) / (1024*1024)\n",
        "        print(f\"ðŸ“¦ File: {os.path.basename(filepath)} ({file_size_mb:.2f} MB)\")\n",
        "        print(f\"â³ Copying to Google Drive...\")\n",
        "\n",
        "        import shutil\n",
        "        shutil.copy2(filepath, dest_path)\n",
        "\n",
        "        print(\"âœ… Upload successful!\")\n",
        "        print(f\"ðŸ“ Location: MyDrive/Merged_Videos/{os.path.basename(filepath)}\")\n",
        "        return dest_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Google Drive error: {str(e)}\")\n",
        "        import traceback\n",
        "        print(traceback.format_exc())\n",
        "        return None\n",
        "\n",
        "# Execute uploads based on user selection\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ðŸ“¤ UPLOAD PROCESS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Track upload results\n",
        "gofile_link = None\n",
        "gdrive_path = None\n",
        "\n",
        "if upload_destination == \"GoFile.io Only\":\n",
        "    gofile_link = upload_to_gofile(UPLOAD_FILE)\n",
        "\n",
        "elif upload_destination == \"Google Drive Only\":\n",
        "    gdrive_path = upload_to_gdrive(UPLOAD_FILE)\n",
        "\n",
        "elif upload_destination == \"Both (GoFile + Google Drive)\":\n",
        "    gofile_link = upload_to_gofile(UPLOAD_FILE)\n",
        "    gdrive_path = upload_to_gdrive(UPLOAD_FILE)\n",
        "\n",
        "else:  # None\n",
        "    print(\"\\nðŸ“ Upload skipped - file saved locally\")\n",
        "    print(f\"ðŸ“„ Location: /content/{UPLOAD_FILE}\")\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ðŸŽ‰ **ALL DONE!**\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nðŸ“Š Summary:\")\n",
        "print(f\"  â€¢ Videos merged: {len(video_info)}\")\n",
        "print(f\"  â€¢ Output file: {output_name}\")\n",
        "print(f\"  â€¢ File size: {os.path.getsize(MERGED_VIDEO) / (1024*1024):.2f} MB\")\n",
        "\n",
        "if gofile_link:\n",
        "    print(f\"\\nðŸ”— GoFile.io Link:\")\n",
        "    print(f\"   {gofile_link}\")\n",
        "\n",
        "if gdrive_path:\n",
        "    print(f\"\\nðŸ“ Google Drive:\")\n",
        "    print(f\"   {gdrive_path}\")\n",
        "\n",
        "if not gofile_link and not gdrive_path and upload_destination != \"None (Keep Local Only)\":\n",
        "    print(f\"\\nâš ï¸ Note: Some uploads may have failed. Check error messages above.\")\n",
        "\n",
        "print(\"\\nâœ¨ Process complete!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhusLs1oFCD292wcaz0BcV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}