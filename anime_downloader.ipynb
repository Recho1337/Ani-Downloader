{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cinichi/Ani-Downloader/blob/main/anime_downloader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VxDjh24kTlGl"
      },
      "outputs": [],
      "source": [
        "\n",
        "# üé¨ AnimeKai Episode Downloader & Merger\n",
        "# Enhanced with chunk downloads, yt-dlp support, and better error handling\n",
        "\n",
        "# @title üîß **Install Dependencies** { display-mode: \"form\" }\n",
        "print(\"üì¶ Installing required packages...\")\n",
        "!pip install -q requests beautifulsoup4 cloudscraper m3u8 pycryptodome tqdm yt-dlp\n",
        "!apt-get -qq install -y ffmpeg aria2 > /dev/null 2>&1\n",
        "print(\"‚úÖ All dependencies installed!\\n\")\n",
        "\n",
        "# @title ‚öôÔ∏è **Configuration** { display-mode: \"form\" }\n",
        "\n",
        "#@markdown ### üîó Anime URL\n",
        "#@markdown Enter the AnimeKai watch URL:\n",
        "anime_url = \"https://animekai.to/watch/jujutsu-kaisen-4gm6\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### üì∫ Episode Selection\n",
        "download_mode = \"Episode Range\" #@param [\"All Episodes\", \"Episode Range\", \"Single Episode\"]\n",
        "\n",
        "#@markdown Single episode number:\n",
        "single_episode = 1 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown Episode range (Start and End):\n",
        "start_episode = 1 #@param {type:\"integer\"}\n",
        "end_episode = 3 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### üé• Quality & Audio Settings\n",
        "video_quality = \"1080p\" #@param [\"1080p\", \"720p\", \"480p\", \"360p\"]\n",
        "prefer_type = \"Soft Sub\" #@param [\"Hard Sub\", \"Soft Sub\", \"Dub & S-Sub\"]\n",
        "prefer_server = \"Server 1\" #@param [\"Server 1\", \"Server 2\"]\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### üì• Download Settings\n",
        "download_method = \"yt-dlp\" #@param [\"yt-dlp\", \"aria2\", \"chunks\", \"ffmpeg\"]\n",
        "\n",
        "#@markdown Chunk size in MB (for chunked downloads):\n",
        "chunk_size_mb = 5 #@param {type:\"slider\", min:1, max:50, step:1}\n",
        "\n",
        "#@markdown Max parallel workers/connections:\n",
        "max_workers = 8 #@param {type:\"slider\", min:1, max:32, step:1}\n",
        "\n",
        "#@markdown Max retry attempts:\n",
        "max_retries = 5 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "\n",
        "#@markdown Timeout in seconds:\n",
        "timeout = 300 #@param {type:\"slider\", min:60, max:600, step:30}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### üîó Merge Settings\n",
        "merge_episodes = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### üì§ Upload Settings\n",
        "upload_to = \"None (Keep Local)\" #@param [\"Google Drive Only\", \"GoFile.io Only\", \"Both\", \"None (Keep Local)\"]\n",
        "\n",
        "print(\"‚úÖ Configuration loaded!\")\n",
        "\n",
        "# @title üåê **Core Functions** { display-mode: \"form\" }\n",
        "\n",
        "import requests\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "import subprocess\n",
        "import threading\n",
        "from bs4 import BeautifulSoup\n",
        "import cloudscraper\n",
        "from urllib.parse import urljoin, urlparse, quote, unquote\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# Create cloudscraper session\n",
        "scraper = cloudscraper.create_scraper(\n",
        "    browser={'browser': 'chrome', 'platform': 'windows', 'desktop': True}\n",
        ")\n",
        "\n",
        "BASE_URL = \"https://animekai.to\"\n",
        "\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
        "    'Referer': BASE_URL,\n",
        "    'Accept': '*/*',\n",
        "    'Accept-Language': 'en-US,en;q=0.9',\n",
        "    'Connection': 'keep-alive'\n",
        "}\n",
        "\n",
        "def enc_dec_request(endpoint, text):\n",
        "    \"\"\"Make request to enc-dec API\"\"\"\n",
        "    try:\n",
        "        url = f\"https://enc-dec.app/api/{endpoint}?text={text}\"\n",
        "        response = scraper.get(url, headers=headers, timeout=30)\n",
        "        data = response.json()\n",
        "        return data.get('result', '')\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Enc-dec error: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_anime_details(url):\n",
        "    \"\"\"Get anime ID and title\"\"\"\n",
        "    try:\n",
        "        response = scraper.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        anime_div = soup.select_one('div[data-id]')\n",
        "        anime_id = anime_div.get('data-id') if anime_div else None\n",
        "\n",
        "        title_elem = soup.select_one('div.title-wrapper h1.title span')\n",
        "        title = title_elem.get('title', '') if title_elem else \"Unknown\"\n",
        "        title = re.sub(r'[<>:\"/\\\\|?*]', '', title)\n",
        "\n",
        "        return anime_id, title\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error getting anime details: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def get_episode_list(anime_id):\n",
        "    \"\"\"Get list of all episodes\"\"\"\n",
        "    try:\n",
        "        enc = enc_dec_request('enc-kai', anime_id)\n",
        "        if not enc:\n",
        "            return []\n",
        "\n",
        "        ep_url = f\"{BASE_URL}/ajax/episodes/list?ani_id={anime_id}&_={enc}\"\n",
        "        response = scraper.get(ep_url, headers=headers)\n",
        "        data = response.json()\n",
        "\n",
        "        if 'result' not in data:\n",
        "            return []\n",
        "\n",
        "        html = data['result']\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "        episodes = []\n",
        "        for ep in soup.select('div.eplist a'):\n",
        "            token = ep.get('token', '')\n",
        "            ep_num = ep.get('num', '0')\n",
        "            langs = ep.get('langs', '0')\n",
        "\n",
        "            langs_int = int(langs) if langs.isdigit() else 0\n",
        "            if langs_int == 1:\n",
        "                subdub = \"Sub\"\n",
        "            elif langs_int == 3:\n",
        "                subdub = \"Dub & Sub\"\n",
        "            else:\n",
        "                subdub = \"\"\n",
        "\n",
        "            episodes.append({\n",
        "                'number': float(ep_num),\n",
        "                'token': token,\n",
        "                'subdub': subdub,\n",
        "                'title': f\"Episode {ep_num}\"\n",
        "            })\n",
        "\n",
        "        return sorted(episodes, key=lambda x: x['number'])\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error getting episodes: {e}\")\n",
        "        return []\n",
        "\n",
        "def get_video_servers(token):\n",
        "    \"\"\"Get available video servers\"\"\"\n",
        "    try:\n",
        "        enc = enc_dec_request('enc-kai', token)\n",
        "        if not enc:\n",
        "            return []\n",
        "\n",
        "        url = f\"{BASE_URL}/ajax/links/list?token={token}&_={enc}\"\n",
        "        response = scraper.get(url, headers=headers)\n",
        "        data = response.json()\n",
        "\n",
        "        if 'result' not in data:\n",
        "            return []\n",
        "\n",
        "        html = data['result']\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "        servers = []\n",
        "        for type_div in soup.select('div.server-items[data-id]'):\n",
        "            type_id = type_div.get('data-id', '')\n",
        "\n",
        "            for server in type_div.select('span.server[data-lid]'):\n",
        "                server_id = server.get('data-lid', '')\n",
        "                server_name = server.text.strip()\n",
        "\n",
        "                servers.append({\n",
        "                    'type': type_id,\n",
        "                    'server_id': server_id,\n",
        "                    'server_name': server_name\n",
        "                })\n",
        "\n",
        "        return servers\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error getting servers: {e}\")\n",
        "        return []\n",
        "\n",
        "def get_video_url(server_id, server_name):\n",
        "    \"\"\"Get direct video URL\"\"\"\n",
        "    try:\n",
        "        enc = enc_dec_request('enc-kai', server_id)\n",
        "        if not enc:\n",
        "            return None\n",
        "\n",
        "        url = f\"{BASE_URL}/ajax/links/view?id={server_id}&_={enc}\"\n",
        "        response = scraper.get(url, headers=headers)\n",
        "        data = response.json()\n",
        "\n",
        "        encoded_link = data.get('result', '')\n",
        "        if not encoded_link:\n",
        "            return None\n",
        "\n",
        "        dec_body = json.dumps({\"text\": encoded_link})\n",
        "        dec_response = scraper.post(\n",
        "            \"https://enc-dec.app/api/dec-kai\",\n",
        "            data=dec_body,\n",
        "            headers={'Content-Type': 'application/json'}\n",
        "        )\n",
        "        dec_data = dec_response.json()\n",
        "        iframe_url = dec_data.get('result', {}).get('url', '')\n",
        "\n",
        "        if not iframe_url:\n",
        "            return None\n",
        "\n",
        "        return extract_megaup_url(iframe_url)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error getting video URL: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_megaup_url(iframe_url):\n",
        "    \"\"\"Extract video URL from MegaUp\"\"\"\n",
        "    try:\n",
        "        parsed = urlparse(iframe_url)\n",
        "        token = parsed.path.split('/')[-1]\n",
        "\n",
        "        media_url = f\"{parsed.scheme}://{parsed.netloc}/media/{token}\"\n",
        "        response = scraper.get(media_url, headers=headers)\n",
        "        data = response.json()\n",
        "        mega_token = data.get('result', '')\n",
        "\n",
        "        if not mega_token:\n",
        "            return None\n",
        "\n",
        "        dec_body = json.dumps({\"text\": mega_token, \"agent\": headers['User-Agent']})\n",
        "        dec_response = scraper.post(\n",
        "            \"https://enc-dec.app/api/dec-mega\",\n",
        "            data=dec_body,\n",
        "            headers={'Content-Type': 'application/json'}\n",
        "        )\n",
        "\n",
        "        mega_data = dec_response.json()\n",
        "        sources = mega_data.get('result', {}).get('sources', [])\n",
        "\n",
        "        if not sources:\n",
        "            return None\n",
        "\n",
        "        return sources[0].get('file', '')\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è MegaUp extraction error: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"‚úÖ Core functions loaded\")\n",
        "\n",
        "# @title üì• **Download Methods** { display-mode: \"form\" }\n",
        "\n",
        "def download_with_ytdlp(url, output_file, episode_num):\n",
        "    \"\"\"Download using yt-dlp (BEST for m3u8)\"\"\"\n",
        "    try:\n",
        "        print(f\"\\nüì• Downloading Episode {episode_num} with yt-dlp...\")\n",
        "        print(f\"   File: {os.path.basename(output_file)}\")\n",
        "\n",
        "        cmd = [\n",
        "            'yt-dlp', url, '-o', output_file,\n",
        "            '--no-warnings', '--no-check-certificate',\n",
        "            '--concurrent-fragments', str(max_workers),\n",
        "            '--retries', str(max_retries),\n",
        "            '--fragment-retries', str(max_retries),\n",
        "            '--socket-timeout', str(timeout),\n",
        "            '--progress', '--newline',\n",
        "            '--user-agent', headers['User-Agent'],\n",
        "            '--referer', BASE_URL\n",
        "        ]\n",
        "\n",
        "        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, universal_newlines=True)\n",
        "\n",
        "        for line in process.stdout:\n",
        "            if '[download]' in line and '%' in line:\n",
        "                match = re.search(r'(\\d+\\.\\d+)%', line)\n",
        "                if match:\n",
        "                    percent = float(match.group(1))\n",
        "                    print(f\"\\r   ‚è≥ Progress: {percent:.1f}%\", end='', flush=True)\n",
        "\n",
        "        process.wait()\n",
        "\n",
        "        if process.returncode == 0 and os.path.exists(output_file):\n",
        "            file_size = os.path.getsize(output_file) / (1024*1024)\n",
        "            print(f\"\\n   ‚úÖ Complete! ({file_size:.2f} MB)\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"\\n   ‚ùå Download failed\")\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        print(f\"\\n   ‚ùå Error: {e}\")\n",
        "        return False\n",
        "\n",
        "def download_with_aria2(url, output_file, episode_num):\n",
        "    \"\"\"Download using aria2c (FAST multi-connection)\"\"\"\n",
        "    try:\n",
        "        print(f\"\\nüì• Downloading Episode {episode_num} with aria2...\")\n",
        "        print(f\"   File: {os.path.basename(output_file)}\")\n",
        "\n",
        "        cmd = [\n",
        "            'aria2c', url,\n",
        "            '-o', os.path.basename(output_file),\n",
        "            '-d', os.path.dirname(output_file),\n",
        "            '-x', str(max_workers),\n",
        "            '-s', str(max_workers),\n",
        "            '--max-tries=5', '--retry-wait=3',\n",
        "            '--user-agent=' + headers['User-Agent'],\n",
        "            '--referer=' + BASE_URL\n",
        "        ]\n",
        "\n",
        "        subprocess.run(cmd, check=False)\n",
        "\n",
        "        if os.path.exists(output_file):\n",
        "            file_size = os.path.getsize(output_file) / (1024*1024)\n",
        "            print(f\"\\n   ‚úÖ Complete! ({file_size:.2f} MB)\")\n",
        "            return True\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"\\n   ‚ùå Error: {e}\")\n",
        "        return False\n",
        "\n",
        "def download_chunk(url, start, end, chunk_file, pbar):\n",
        "    \"\"\"Download a single chunk\"\"\"\n",
        "    try:\n",
        "        chunk_headers = headers.copy()\n",
        "        chunk_headers['Range'] = f'bytes={start}-{end}'\n",
        "        response = scraper.get(url, headers=chunk_headers, stream=True, timeout=30)\n",
        "\n",
        "        if response.status_code not in [200, 206]:\n",
        "            return False\n",
        "\n",
        "        with open(chunk_file, 'wb') as f:\n",
        "            for data in response.iter_content(chunk_size=8192):\n",
        "                if data:\n",
        "                    f.write(data)\n",
        "                    pbar.update(len(data))\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def download_with_chunks(url, output_file, episode_num):\n",
        "    \"\"\"Download with chunked/parallel downloading\"\"\"\n",
        "    try:\n",
        "        print(f\"\\nüì• Downloading Episode {episode_num} with chunks...\")\n",
        "        print(f\"   File: {os.path.basename(output_file)}\")\n",
        "\n",
        "        head_response = scraper.head(url, headers=headers, timeout=10)\n",
        "\n",
        "        if 'Content-Length' not in head_response.headers:\n",
        "            return download_direct(url, output_file, episode_num)\n",
        "\n",
        "        total_size = int(head_response.headers['Content-Length'])\n",
        "        chunk_size = chunk_size_mb * 1024 * 1024\n",
        "\n",
        "        chunks = []\n",
        "        for i in range(0, total_size, chunk_size):\n",
        "            start = i\n",
        "            end = min(i + chunk_size - 1, total_size - 1)\n",
        "            chunks.append((start, end))\n",
        "\n",
        "        chunk_dir = f\"{output_file}.chunks\"\n",
        "        os.makedirs(chunk_dir, exist_ok=True)\n",
        "        chunk_files = []\n",
        "\n",
        "        with tqdm(total=total_size, unit='B', unit_scale=True, desc=\"   ‚è≥ Downloading\", ncols=80) as pbar:\n",
        "            with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "                futures = {}\n",
        "                for idx, (start, end) in enumerate(chunks):\n",
        "                    chunk_file = f\"{chunk_dir}/chunk_{idx:04d}\"\n",
        "                    chunk_files.append(chunk_file)\n",
        "                    future = executor.submit(download_chunk, url, start, end, chunk_file, pbar)\n",
        "                    futures[future] = idx\n",
        "\n",
        "                for future in as_completed(futures):\n",
        "                    if not future.result():\n",
        "                        shutil.rmtree(chunk_dir, ignore_errors=True)\n",
        "                        return False\n",
        "\n",
        "        print(\"   üîó Merging chunks...\")\n",
        "        with open(output_file, 'wb') as outfile:\n",
        "            for chunk_file in chunk_files:\n",
        "                with open(chunk_file, 'rb') as infile:\n",
        "                    shutil.copyfileobj(infile, outfile)\n",
        "\n",
        "        shutil.rmtree(chunk_dir, ignore_errors=True)\n",
        "\n",
        "        file_size = os.path.getsize(output_file) / (1024*1024)\n",
        "        print(f\"   ‚úÖ Complete! ({file_size:.2f} MB)\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"\\n   ‚ùå Error: {e}\")\n",
        "        return False\n",
        "\n",
        "def download_direct(url, output_file, episode_num):\n",
        "    \"\"\"Direct download with progress\"\"\"\n",
        "    try:\n",
        "        print(f\"\\nüì• Downloading Episode {episode_num}...\")\n",
        "        response = scraper.get(url, headers=headers, stream=True, timeout=30)\n",
        "        total_size = int(response.headers.get('content-length', 0))\n",
        "\n",
        "        with open(output_file, 'wb') as f:\n",
        "            with tqdm(total=total_size, unit='B', unit_scale=True, desc=\"   ‚è≥\", ncols=80) as pbar:\n",
        "                for chunk in response.iter_content(chunk_size=8192):\n",
        "                    if chunk:\n",
        "                        f.write(chunk)\n",
        "                        pbar.update(len(chunk))\n",
        "\n",
        "        file_size = os.path.getsize(output_file) / (1024*1024)\n",
        "        print(f\"   ‚úÖ Complete! ({file_size:.2f} MB)\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"\\n   ‚ùå Error: {e}\")\n",
        "        return False\n",
        "\n",
        "def download_episode(url, output_file, episode_num):\n",
        "    \"\"\"Main download with retry\"\"\"\n",
        "    is_m3u8 = '.m3u8' in url\n",
        "\n",
        "    if download_method == \"yt-dlp\":\n",
        "        download_func = download_with_ytdlp\n",
        "    elif download_method == \"aria2\" and not is_m3u8:\n",
        "        download_func = download_with_aria2\n",
        "    elif download_method == \"chunks\" and not is_m3u8:\n",
        "        download_func = download_with_chunks\n",
        "    else:\n",
        "        download_func = download_with_ytdlp if is_m3u8 else download_with_chunks\n",
        "\n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        if attempt > 1:\n",
        "            print(f\"\\n   üîÑ Retry {attempt}/{max_retries}...\")\n",
        "            time.sleep(3)\n",
        "\n",
        "        if os.path.exists(output_file):\n",
        "            os.remove(output_file)\n",
        "\n",
        "        if download_func(url, output_file, episode_num):\n",
        "            return True\n",
        "\n",
        "    return False\n",
        "\n",
        "print(\"‚úÖ Download methods loaded\")\n",
        "\n",
        "# @title üì∫ **Fetch Anime Info & Download** { display-mode: \"form\" }\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üé¨ ANIMEKAI EPISODE DOWNLOADER\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\nüîç Processing: {anime_url}\")\n",
        "\n",
        "anime_id, anime_title = get_anime_details(anime_url)\n",
        "\n",
        "if not anime_id:\n",
        "    raise Exception(\"‚ùå Could not extract anime ID\")\n",
        "\n",
        "print(f\"‚úÖ Anime ID: {anime_id}\")\n",
        "print(f\"üì∫ Title: {anime_title}\")\n",
        "\n",
        "episode_list = get_episode_list(anime_id)\n",
        "\n",
        "if not episode_list:\n",
        "    raise Exception(\"‚ùå No episodes found!\")\n",
        "\n",
        "print(f\"üìã Found {len(episode_list)} episode(s)\")\n",
        "\n",
        "# Determine episodes\n",
        "if download_mode == \"Single Episode\":\n",
        "    episodes_to_download = [ep for ep in episode_list if ep['number'] == single_episode]\n",
        "elif download_mode == \"Episode Range\":\n",
        "    episodes_to_download = [ep for ep in episode_list if start_episode <= ep['number'] <= end_episode]\n",
        "else:\n",
        "    episodes_to_download = episode_list\n",
        "\n",
        "if not episodes_to_download:\n",
        "    raise Exception(\"‚ùå No episodes match selection!\")\n",
        "\n",
        "print(f\"üì• Will download {len(episodes_to_download)} episode(s)\")\n",
        "\n",
        "download_dir = f\"downloads/{anime_title}\"\n",
        "os.makedirs(download_dir, exist_ok=True)\n",
        "\n",
        "type_map = {\"Hard Sub\": \"sub\", \"Soft Sub\": \"softsub\", \"Dub & S-Sub\": \"dub\"}\n",
        "prefer_type_id = type_map.get(prefer_type, \"softsub\")\n",
        "\n",
        "downloaded_files = []\n",
        "failed_episodes = []\n",
        "\n",
        "for idx, episode in enumerate(episodes_to_download, 1):\n",
        "    print(f\"\\n[{idx}/{len(episodes_to_download)}] Episode {episode['number']}\")\n",
        "\n",
        "    try:\n",
        "        servers = get_video_servers(episode['token'])\n",
        "        if not servers:\n",
        "            failed_episodes.append(episode['number'])\n",
        "            continue\n",
        "\n",
        "        # Map preference types\n",
        "        # Note: \"dub\" type on AnimeKai means Dub & S-Sub (dual audio with soft subs)\n",
        "        type_map_search = {\n",
        "            \"Hard Sub\": \"sub\",\n",
        "            \"Soft Sub\": \"softsub\",\n",
        "            \"Dub & S-Sub\": \"dub\"  # This is the dual audio option\n",
        "        }\n",
        "        prefer_type_id = type_map_search.get(prefer_type, \"softsub\")\n",
        "\n",
        "        # Filter servers by preference\n",
        "        matching_servers = [s for s in servers if s['type'] == prefer_type_id and s['server_name'] == prefer_server]\n",
        "\n",
        "        # Fallback 1: Try any server with matching type\n",
        "        if not matching_servers:\n",
        "            matching_servers = [s for s in servers if s['type'] == prefer_type_id]\n",
        "\n",
        "        # Fallback 2: Try any server with preferred server name\n",
        "        if not matching_servers:\n",
        "            matching_servers = [s for s in servers if s['server_name'] == prefer_server]\n",
        "\n",
        "        # Fallback 3: Use first available server\n",
        "        if not matching_servers:\n",
        "            matching_servers = servers[:1]\n",
        "\n",
        "        server = matching_servers[0]\n",
        "\n",
        "        # Show what type we're actually downloading\n",
        "        type_display = {\n",
        "            \"sub\": \"Hard Sub\",\n",
        "            \"softsub\": \"Soft Sub\",\n",
        "            \"dub\": \"Dub & S-Sub (Dual Audio)\"\n",
        "        }.get(server['type'], server['type'])\n",
        "\n",
        "        print(f\"   üé• Server: {server['server_name']} | Type: {type_display}\")\n",
        "\n",
        "        video_url = get_video_url(server['server_id'], server['server_name'])\n",
        "        if not video_url:\n",
        "            failed_episodes.append(episode['number'])\n",
        "            continue\n",
        "\n",
        "        ep_num_str = f\"{int(episode['number']):03d}\"\n",
        "\n",
        "        # Add type to filename for clarity\n",
        "        type_suffix = {\n",
        "            \"sub\": \"HardSub\",\n",
        "            \"softsub\": \"SoftSub\",\n",
        "            \"dub\": \"DualAudio\"\n",
        "        }.get(server['type'], \"\")\n",
        "\n",
        "        filename = f\"{download_dir}/Episode_{ep_num_str}_{type_suffix}.mp4\"\n",
        "\n",
        "        if download_episode(video_url, filename, episode['number']):\n",
        "            downloaded_files.append(filename)\n",
        "        else:\n",
        "            failed_episodes.append(episode['number'])\n",
        "\n",
        "        time.sleep(2)\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error: {e}\")\n",
        "        failed_episodes.append(episode['number'])\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üìä DOWNLOAD SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\n‚úÖ Downloaded: {len(downloaded_files)} episode(s)\")\n",
        "if failed_episodes:\n",
        "    print(f\"‚ùå Failed: {', '.join(map(str, failed_episodes))}\")\n",
        "if downloaded_files:\n",
        "    total_size = sum(os.path.getsize(f) for f in downloaded_files) / (1024*1024)\n",
        "    print(f\"üíæ Total: {total_size:.2f} MB\")\n",
        "print(\"\\nüéâ COMPLETE!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7PZkspKUd_a"
      },
      "outputs": [],
      "source": [
        "\n",
        "# üé¨ Video Episode Merger & Uploader\n",
        "# Download ZIP file with video episodes, extract, merge them in order, and upload\n",
        "\n",
        "# @title üîß **Install Dependencies** { display-mode: \"form\" }\n",
        "print(\"üì¶ Installing required packages...\")\n",
        "!pip install -q natsort requests\n",
        "!apt-get -qq install -y ffmpeg > /dev/null 2>&1\n",
        "print(\"‚úÖ All dependencies installed!\\n\")\n",
        "\n",
        "# @title ‚öôÔ∏è **Configuration** { display-mode: \"form\" }\n",
        "\n",
        "#@markdown ### üì• Download Settings\n",
        "#@markdown Enter the direct download link (DDL) for your ZIP file:\n",
        "zip_url = \"https://example.com/videos.zip\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Custom User-Agent (leave default if unsure):\n",
        "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### üé¨ Merge Settings\n",
        "#@markdown Custom output name (leave empty to auto-detect from episodes):\n",
        "custom_output_name = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Video quality for merge:\n",
        "merge_quality = \"Copy Original (Fastest)\" #@param [\"Copy Original (Fastest)\", \"Re-encode High Quality\", \"Re-encode Compressed\"]\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### üì§ Upload Settings\n",
        "upload_destination = \"Both (GoFile + Google Drive)\" #@param [\"GoFile.io Only\", \"Google Drive Only\", \"Both (GoFile + Google Drive)\", \"None (Keep Local Only)\"]\n",
        "\n",
        "#@markdown Create ZIP of merged video for upload?\n",
        "create_upload_zip = False #@param {type:\"boolean\"}\n",
        "\n",
        "print(\"‚úÖ Configuration set!\")\n",
        "\n",
        "# @title üì• **Download ZIP File** { display-mode: \"form\" }\n",
        "import requests\n",
        "import os\n",
        "import re\n",
        "from urllib.parse import unquote, urlparse\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"üîΩ Starting download...\")\n",
        "print(f\"üîó URL: {zip_url[:60]}...\" if len(zip_url) > 60 else f\"üîó URL: {zip_url}\")\n",
        "\n",
        "headers = {'User-Agent': user_agent}\n",
        "\n",
        "try:\n",
        "    response = requests.get(zip_url, headers=headers, stream=True, allow_redirects=True)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    # Smart filename detection\n",
        "    zip_filename = None\n",
        "\n",
        "    # Method 1: Content-Disposition header\n",
        "    if 'Content-Disposition' in response.headers:\n",
        "        cd = response.headers['Content-Disposition']\n",
        "        filenames = re.findall(r'filename\\*?=[\"\\']?(?:UTF-8\\'\\')?([^\"\\';]+)[\"\\']?', cd)\n",
        "        if filenames:\n",
        "            zip_filename = unquote(filenames[0])\n",
        "            print(f\"üìã Filename from header: {zip_filename}\")\n",
        "\n",
        "    # Method 2: Final URL after redirects\n",
        "    if not zip_filename:\n",
        "        final_url = response.url\n",
        "        url_path = urlparse(final_url).path\n",
        "        zip_filename = os.path.basename(url_path)\n",
        "        zip_filename = unquote(zip_filename)\n",
        "        print(f\"üìã Filename from URL: {zip_filename}\")\n",
        "\n",
        "    # Method 3: Extract meaningful name from URL\n",
        "    if not zip_filename or zip_filename in ['', 'download', 'file']:\n",
        "        # Try to extract from full URL path\n",
        "        url_parts = [p for p in urlparse(zip_url).path.split('/') if p and p != 'download']\n",
        "        if url_parts:\n",
        "            zip_filename = url_parts[-1]\n",
        "            zip_filename = unquote(zip_filename)\n",
        "\n",
        "    # Ensure .zip extension\n",
        "    if not zip_filename.lower().endswith('.zip'):\n",
        "        if '.' not in zip_filename:\n",
        "            zip_filename += '.zip'\n",
        "        else:\n",
        "            zip_filename = os.path.splitext(zip_filename)[0] + '.zip'\n",
        "\n",
        "    # Clean filename (remove invalid characters)\n",
        "    zip_filename = re.sub(r'[<>:\"|?*\\\\]', '_', zip_filename)\n",
        "    zip_filename = re.sub(r'[\\x00-\\x1f]', '', zip_filename)  # Remove control characters\n",
        "\n",
        "    print(f\"üíæ Saving as: {zip_filename}\")\n",
        "\n",
        "    total_size = int(response.headers.get('content-length', 0))\n",
        "    downloaded = 0\n",
        "\n",
        "    with open(zip_filename, 'wb') as f:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            if chunk:\n",
        "                f.write(chunk)\n",
        "                downloaded += len(chunk)\n",
        "                if total_size:\n",
        "                    percent = (downloaded / total_size) * 100\n",
        "                    mb_downloaded = downloaded / (1024*1024)\n",
        "                    mb_total = total_size / (1024*1024)\n",
        "                    print(f\"\\r‚è≥ Progress: {percent:.1f}% ({mb_downloaded:.1f}/{mb_total:.1f} MB)\", end='')\n",
        "\n",
        "    print(f\"\\n‚úÖ Downloaded: {zip_filename} ({downloaded / (1024*1024):.2f} MB)\")\n",
        "\n",
        "    # Extract base name for later use\n",
        "    ZIP_BASE_NAME = os.path.splitext(zip_filename)[0]\n",
        "    ZIP_BASE_NAME = re.sub(r'[_\\-\\s]+', ' ', ZIP_BASE_NAME).strip()\n",
        "\n",
        "    print(f\"üì¶ Base name extracted: '{ZIP_BASE_NAME}'\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Download failed: {str(e)}\")\n",
        "    import traceback\n",
        "    print(traceback.format_exc())\n",
        "    raise\n",
        "\n",
        "# @title üì¶ **Extract ZIP File** { display-mode: \"form\" }\n",
        "import zipfile\n",
        "\n",
        "extract_folder = \"extracted_videos\"\n",
        "os.makedirs(extract_folder, exist_ok=True)\n",
        "\n",
        "print(f\"\\nüìÇ Extracting to: {extract_folder}/\")\n",
        "print(\"‚è≥ Please wait...\")\n",
        "\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "        file_list = zip_ref.namelist()\n",
        "        total_files = len(file_list)\n",
        "\n",
        "        print(f\"üìã Found {total_files} file(s) in ZIP\\n\")\n",
        "\n",
        "        # Extract with progress\n",
        "        for idx, file in enumerate(file_list, 1):\n",
        "            zip_ref.extract(file, extract_folder)\n",
        "            if idx % 5 == 0 or idx == total_files:\n",
        "                print(f\"\\r‚è≥ Extracting: {idx}/{total_files} files...\", end='')\n",
        "\n",
        "        print(f\"\\n\\nüìÑ Extracted files:\")\n",
        "        video_count = 0\n",
        "        for file in file_list:\n",
        "            file_lower = file.lower()\n",
        "            is_video = any(file_lower.endswith(ext) for ext in ['.mp4', '.mkv', '.avi', '.mov', '.flv', '.wmv', '.webm', '.m4v'])\n",
        "            icon = \"üé¨\" if is_video else \"üìÑ\"\n",
        "            print(f\"  {icon} {file}\")\n",
        "            if is_video:\n",
        "                video_count += 1\n",
        "\n",
        "        print(f\"\\n‚úÖ Extraction complete! Found {video_count} video file(s)\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Extraction failed: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# @title üîç **Detect and Sort Episodes** { display-mode: \"form\" }\n",
        "import re\n",
        "from natsort import natsorted\n",
        "\n",
        "def extract_episode_info(filename):\n",
        "    \"\"\"Enhanced episode detection with better pattern matching\"\"\"\n",
        "    name = os.path.basename(filename)\n",
        "\n",
        "    # Combined season and episode patterns (S01E01, S1E1, etc.)\n",
        "    combined_patterns = [\n",
        "        r'[Ss](\\d+)[Ee](\\d+)',  # S01E01, S1E1\n",
        "        r'[Ss]eason[\\s._-]*(\\d+)[\\s._-]*[Ee]pisode[\\s._-]*(\\d+)',  # Season 1 Episode 1\n",
        "        r'[Ss]eason[\\s._-]*(\\d+)[\\s._-]*[Ee][Pp][\\s._-]*(\\d+)',  # Season 1 Ep 1\n",
        "        r'(\\d+)[xX](\\d+)',  # 1x01\n",
        "    ]\n",
        "\n",
        "    # Try combined patterns first\n",
        "    for pattern in combined_patterns:\n",
        "        match = re.search(pattern, name, re.IGNORECASE)\n",
        "        if match:\n",
        "            return int(match.group(1)), int(match.group(2))\n",
        "\n",
        "    # Separate season patterns\n",
        "    season_patterns = [\n",
        "        r'[Ss]eason[\\s._-]*(\\d+)',\n",
        "        r'[Ss](\\d+)(?![Ee])',  # S1 but not followed by E\n",
        "        r'Season[\\s._-]*(\\d+)',\n",
        "    ]\n",
        "\n",
        "    # Episode patterns\n",
        "    episode_patterns = [\n",
        "        r'[Ee]pisode[\\s._-]*(\\d+)',\n",
        "        r'[Ee][Pp][\\s._-]*(\\d+)',\n",
        "        r'[Ee](\\d+)',\n",
        "        r'Episode[\\s._-]*(\\d+)',\n",
        "        r'[\\s._-](\\d{1,3})[\\s._-]',  # Number surrounded by separators\n",
        "        r'^(\\d{1,3})[\\s._-]',  # Number at start\n",
        "        r'[\\s._-](\\d{1,3})\\.',  # Number before extension\n",
        "    ]\n",
        "\n",
        "    season = None\n",
        "    episode = None\n",
        "\n",
        "    # Find season\n",
        "    for pattern in season_patterns:\n",
        "        match = re.search(pattern, name, re.IGNORECASE)\n",
        "        if match:\n",
        "            season = int(match.group(1))\n",
        "            break\n",
        "\n",
        "    # Find episode\n",
        "    for pattern in episode_patterns:\n",
        "        match = re.search(pattern, name, re.IGNORECASE)\n",
        "        if match:\n",
        "            ep_num = int(match.group(1))\n",
        "            # Reasonable episode number (1-999)\n",
        "            if 1 <= ep_num <= 999:\n",
        "                episode = ep_num\n",
        "                break\n",
        "\n",
        "    return season, episode\n",
        "\n",
        "# Find all video files\n",
        "video_extensions = ['.mp4', '.mkv', '.avi', '.mov', '.flv', '.wmv', '.webm', '.m4v', '.ts', '.m2ts']\n",
        "video_files = []\n",
        "\n",
        "for root, dirs, files in os.walk(extract_folder):\n",
        "    for file in files:\n",
        "        if any(file.lower().endswith(ext) for ext in video_extensions):\n",
        "            full_path = os.path.join(root, file)\n",
        "            video_files.append(full_path)\n",
        "\n",
        "if not video_files:\n",
        "    print(\"‚ùå No video files found in the ZIP!\")\n",
        "    raise Exception(\"No video files detected\")\n",
        "\n",
        "print(f\"üé¨ Found {len(video_files)} video file(s)\\n\")\n",
        "\n",
        "# Extract info and sort\n",
        "video_info = []\n",
        "for vf in video_files:\n",
        "    season, episode = extract_episode_info(vf)\n",
        "    video_info.append({\n",
        "        'path': vf,\n",
        "        'name': os.path.basename(vf),\n",
        "        'season': season if season else 0,\n",
        "        'episode': episode if episode else 0\n",
        "    })\n",
        "\n",
        "# Sort by season, then episode, then natural name\n",
        "video_info.sort(key=lambda x: (x['season'], x['episode'], x['name']))\n",
        "\n",
        "print(\"üìã **Detected Episode Order:**\")\n",
        "print(\"=\" * 70)\n",
        "for idx, info in enumerate(video_info, 1):\n",
        "    s_info = f\"S{info['season']:02d}\" if info['season'] else \"S??\"\n",
        "    e_info = f\"E{info['episode']:02d}\" if info['episode'] else \"E??\"\n",
        "    size_mb = os.path.getsize(info['path']) / (1024*1024)\n",
        "    print(f\"{idx:2d}. [{s_info}{e_info}] {info['name'][:45]:<45} ({size_mb:.1f} MB)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# @title üéûÔ∏è **Merge Videos** { display-mode: \"form\" }\n",
        "import subprocess\n",
        "\n",
        "print(\"\\nüé¨ Preparing to merge videos...\")\n",
        "\n",
        "# Create file list for ffmpeg\n",
        "list_file = \"filelist.txt\"\n",
        "with open(list_file, 'w', encoding='utf-8') as f:\n",
        "    for info in video_info:\n",
        "        # Escape single quotes for ffmpeg\n",
        "        safe_path = info['path'].replace(\"'\", \"'\\\\''\")\n",
        "        f.write(f\"file '{safe_path}'\\n\")\n",
        "\n",
        "print(f\"‚úÖ Created merge list with {len(video_info)} video(s)\")\n",
        "\n",
        "# Determine output filename\n",
        "if custom_output_name:\n",
        "    output_name = custom_output_name\n",
        "    if not output_name.lower().endswith('.mp4'):\n",
        "        output_name += '.mp4'\n",
        "else:\n",
        "    # Auto-generate name\n",
        "    seasons = [v['season'] for v in video_info if v['season'] > 0]\n",
        "    episodes = [v['episode'] for v in video_info if v['episode'] > 0]\n",
        "\n",
        "    base_name = ZIP_BASE_NAME\n",
        "\n",
        "    if seasons and episodes:\n",
        "        min_season = min(seasons)\n",
        "        max_season = max(seasons)\n",
        "        min_episode = min(episodes)\n",
        "        max_episode = max(episodes)\n",
        "\n",
        "        if min_season == max_season:\n",
        "            output_name = f\"{base_name} Season {min_season:02d} Episodes {min_episode:02d}-{max_episode:02d}.mp4\"\n",
        "        else:\n",
        "            output_name = f\"{base_name} S{min_season:02d}-S{max_season:02d} Ep{min_episode:02d}-{max_episode:02d}.mp4\"\n",
        "    else:\n",
        "        output_name = f\"{base_name} Merged Complete.mp4\"\n",
        "\n",
        "# Clean output name\n",
        "output_name = re.sub(r'[<>:\"|?*\\\\]', '_', output_name)\n",
        "output_name = re.sub(r'\\s+', ' ', output_name).strip()\n",
        "\n",
        "print(f\"\\nüìÅ Output filename: {output_name}\")\n",
        "\n",
        "# Build ffmpeg command based on quality setting\n",
        "if merge_quality == \"Copy Original (Fastest)\":\n",
        "    cmd = [\n",
        "        'ffmpeg', '-f', 'concat', '-safe', '0', '-i', list_file,\n",
        "        '-c', 'copy', output_name, '-y'\n",
        "    ]\n",
        "    print(\"‚ö° Mode: Fast merge (copy streams, no re-encoding)\")\n",
        "elif merge_quality == \"Re-encode High Quality\":\n",
        "    cmd = [\n",
        "        'ffmpeg', '-f', 'concat', '-safe', '0', '-i', list_file,\n",
        "        '-c:v', 'libx264', '-crf', '18', '-preset', 'slow',\n",
        "        '-c:a', 'aac', '-b:a', '192k',\n",
        "        output_name, '-y'\n",
        "    ]\n",
        "    print(\"üé® Mode: High quality re-encode (slower, best quality)\")\n",
        "else:  # Compressed\n",
        "    cmd = [\n",
        "        'ffmpeg', '-f', 'concat', '-safe', '0', '-i', list_file,\n",
        "        '-c:v', 'libx264', '-crf', '23', '-preset', 'medium',\n",
        "        '-c:a', 'aac', '-b:a', '128k',\n",
        "        output_name, '-y'\n",
        "    ]\n",
        "    print(\"üì¶ Mode: Compressed re-encode (smaller file size)\")\n",
        "\n",
        "print(\"\\n‚è≥ Merging videos... This may take a while.\\n\")\n",
        "\n",
        "try:\n",
        "    # Run ffmpeg\n",
        "    process = subprocess.Popen(cmd, stderr=subprocess.PIPE, universal_newlines=True)\n",
        "\n",
        "    # Parse ffmpeg output for progress\n",
        "    duration_pattern = re.compile(r'Duration: (\\d{2}):(\\d{2}):(\\d{2})')\n",
        "    time_pattern = re.compile(r'time=(\\d{2}):(\\d{2}):(\\d{2})')\n",
        "\n",
        "    total_duration = None\n",
        "\n",
        "    for line in process.stderr:\n",
        "        # Get total duration\n",
        "        if total_duration is None:\n",
        "            dur_match = duration_pattern.search(line)\n",
        "            if dur_match:\n",
        "                h, m, s = map(int, dur_match.groups())\n",
        "                total_duration = h * 3600 + m * 60 + s\n",
        "\n",
        "        # Get current time\n",
        "        time_match = time_pattern.search(line)\n",
        "        if time_match and total_duration:\n",
        "            h, m, s = map(int, time_match.groups())\n",
        "            current_time = h * 3600 + m * 60 + s\n",
        "            percent = (current_time / total_duration) * 100\n",
        "            print(f\"\\rüé¨ Progress: {percent:.1f}% ({current_time//60}:{current_time%60:02d} / {total_duration//60}:{total_duration%60:02d})\", end='')\n",
        "\n",
        "    process.wait()\n",
        "\n",
        "    if process.returncode == 0:\n",
        "        file_size = os.path.getsize(output_name) / (1024*1024)\n",
        "        print(f\"\\n\\n‚úÖ **Merge Complete!**\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"üìÅ Output: {output_name}\")\n",
        "        print(f\"üíæ Size: {file_size:.2f} MB\")\n",
        "        print(f\"üé¨ Episodes: {len(video_info)}\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        MERGED_VIDEO = output_name\n",
        "    else:\n",
        "        print(f\"\\n‚ùå Merge failed with exit code {process.returncode}\")\n",
        "        raise Exception(\"FFmpeg merge failed\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Error during merge: {str(e)}\")\n",
        "    raise\n",
        "finally:\n",
        "    # Cleanup\n",
        "    if os.path.exists(list_file):\n",
        "        os.remove(list_file)\n",
        "\n",
        "# @title üì¶ **Create ZIP of Merged Video (Optional)** { display-mode: \"form\" }\n",
        "\n",
        "if create_upload_zip:\n",
        "    print(\"\\nüì¶ Creating ZIP file of merged video...\")\n",
        "\n",
        "    zip_output = output_name.replace('.mp4', '.zip')\n",
        "\n",
        "    import zipfile\n",
        "    with zipfile.ZipFile(zip_output, 'w', zipfile.ZIP_DEFLATED, compresslevel=0) as zipf:\n",
        "        print(f\"‚è≥ Adding {output_name} to ZIP...\")\n",
        "        zipf.write(output_name, os.path.basename(output_name))\n",
        "\n",
        "    zip_size = os.path.getsize(zip_output) / (1024*1024)\n",
        "    print(f\"‚úÖ ZIP created: {zip_output} ({zip_size:.2f} MB)\")\n",
        "\n",
        "    UPLOAD_FILE = zip_output\n",
        "else:\n",
        "    UPLOAD_FILE = MERGED_VIDEO\n",
        "    print(\"\\nüìÑ Will upload video file directly (no ZIP)\")\n",
        "\n",
        "# @title üì§ **Upload Files** { display-mode: \"form\" }\n",
        "\n",
        "def upload_to_gofile(filepath):\n",
        "    \"\"\"Upload file to GoFile.io\"\"\"\n",
        "    try:\n",
        "        print(\"\\nüåê GoFile.io Upload\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Get best server\n",
        "        server_response = requests.get('https://api.gofile.io/servers', timeout=30)\n",
        "        server_response.raise_for_status()\n",
        "        server_data = server_response.json()\n",
        "\n",
        "        if server_data['status'] != 'ok':\n",
        "            print(\"‚ùå Failed to get GoFile server\")\n",
        "            return None\n",
        "\n",
        "        server = server_data['data']['servers'][0]['name']\n",
        "        print(f\"üì° Server: {server}\")\n",
        "\n",
        "        # Updated endpoint\n",
        "        upload_url = f'https://{server}.gofile.io/contents/uploadfile'\n",
        "\n",
        "        file_size_mb = os.path.getsize(filepath) / (1024*1024)\n",
        "        print(f\"üì¶ File: {os.path.basename(filepath)} ({file_size_mb:.2f} MB)\")\n",
        "        print(\"‚è≥ Uploading... (this may take several minutes for large files)\")\n",
        "\n",
        "        with open(filepath, 'rb') as f:\n",
        "            files_data = {'file': (os.path.basename(filepath), f, 'application/octet-stream')}\n",
        "            response = requests.post(upload_url, files=files_data, timeout=7200)\n",
        "\n",
        "        response.raise_for_status()\n",
        "\n",
        "        result = response.json()\n",
        "        if result['status'] == 'ok':\n",
        "            download_page = result['data']['downloadPage']\n",
        "            print(\"‚úÖ Upload successful!\")\n",
        "            print(f\"üîó Link: {download_page}\")\n",
        "            return download_page\n",
        "        else:\n",
        "            print(f\"‚ùå Upload failed: {result.get('message', 'Unknown error')}\")\n",
        "            return None\n",
        "\n",
        "    except requests.exceptions.Timeout:\n",
        "        print(\"‚ùå Upload timed out - file may be too large for GoFile\")\n",
        "        return None\n",
        "    except requests.exceptions.JSONDecodeError:\n",
        "        print(\"‚ùå Invalid response from GoFile - service may be down\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå GoFile error: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def upload_to_gdrive(filepath):\n",
        "    \"\"\"Upload file to Google Drive\"\"\"\n",
        "    try:\n",
        "        print(\"\\n‚òÅÔ∏è Google Drive Upload\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        from google.colab import drive\n",
        "\n",
        "        # Check if already mounted\n",
        "        if not os.path.exists('/content/drive/MyDrive'):\n",
        "            drive.mount('/content/drive', force_remount=False)\n",
        "            print(\"‚úÖ Google Drive mounted!\")\n",
        "        else:\n",
        "            print(\"‚úÖ Google Drive already mounted!\")\n",
        "\n",
        "        destination = '/content/drive/MyDrive/Merged_Videos/'\n",
        "        os.makedirs(destination, exist_ok=True)\n",
        "\n",
        "        dest_path = os.path.join(destination, os.path.basename(filepath))\n",
        "\n",
        "        # Check if file exists\n",
        "        if not os.path.exists(filepath):\n",
        "            print(f\"‚ùå Source file not found: {filepath}\")\n",
        "            return None\n",
        "\n",
        "        file_size_mb = os.path.getsize(filepath) / (1024*1024)\n",
        "        print(f\"üì¶ File: {os.path.basename(filepath)} ({file_size_mb:.2f} MB)\")\n",
        "        print(f\"‚è≥ Copying to Google Drive...\")\n",
        "\n",
        "        import shutil\n",
        "        shutil.copy2(filepath, dest_path)\n",
        "\n",
        "        print(\"‚úÖ Upload successful!\")\n",
        "        print(f\"üìÅ Location: MyDrive/Merged_Videos/{os.path.basename(filepath)}\")\n",
        "        return dest_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Google Drive error: {str(e)}\")\n",
        "        import traceback\n",
        "        print(traceback.format_exc())\n",
        "        return None\n",
        "\n",
        "# Execute uploads based on user selection\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üì§ UPLOAD PROCESS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Track upload results\n",
        "gofile_link = None\n",
        "gdrive_path = None\n",
        "\n",
        "if upload_destination == \"GoFile.io Only\":\n",
        "    gofile_link = upload_to_gofile(UPLOAD_FILE)\n",
        "\n",
        "elif upload_destination == \"Google Drive Only\":\n",
        "    gdrive_path = upload_to_gdrive(UPLOAD_FILE)\n",
        "\n",
        "elif upload_destination == \"Both (GoFile + Google Drive)\":\n",
        "    gofile_link = upload_to_gofile(UPLOAD_FILE)\n",
        "    gdrive_path = upload_to_gdrive(UPLOAD_FILE)\n",
        "\n",
        "else:  # None\n",
        "    print(\"\\nüìÅ Upload skipped - file saved locally\")\n",
        "    print(f\"üìÑ Location: /content/{UPLOAD_FILE}\")\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üéâ **ALL DONE!**\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nüìä Summary:\")\n",
        "print(f\"  ‚Ä¢ Videos merged: {len(video_info)}\")\n",
        "print(f\"  ‚Ä¢ Output file: {output_name}\")\n",
        "print(f\"  ‚Ä¢ File size: {os.path.getsize(MERGED_VIDEO) / (1024*1024):.2f} MB\")\n",
        "\n",
        "if gofile_link:\n",
        "    print(f\"\\nüîó GoFile.io Link:\")\n",
        "    print(f\"   {gofile_link}\")\n",
        "\n",
        "if gdrive_path:\n",
        "    print(f\"\\nüìÅ Google Drive:\")\n",
        "    print(f\"   {gdrive_path}\")\n",
        "\n",
        "if not gofile_link and not gdrive_path and upload_destination != \"None (Keep Local Only)\":\n",
        "    print(f\"\\n‚ö†Ô∏è Note: Some uploads may have failed. Check error messages above.\")\n",
        "\n",
        "print(\"\\n‚ú® Process complete!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhLJ83Ih/dZsUNwKJwHyaA",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}